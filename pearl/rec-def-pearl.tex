\PassOptionsToPackage{dvipsnames}{xcolor}
\documentclass[manuscript,review,screen,acmsmall]{acmart}

\newif\ifpure
\puretrue

\usepackage[capitalise,nameinlink,noabbrev]{cleveref}
\usepackage{textgreek}
\usepackage{afterpage}
\usepackage{tikz}

% LHS2TeX setup

\let\Bbbk\undefined %https://github.com/kosmikus/lhs2tex/issues/82
\typeout{(./polytt.fmt)} % For latexmk
%include polytt.fmt
%format IND = "~~"
%format -> = "–>"
%format <- = "<–"
%format (QUOTETYPE(x)) = "\textquotesingle\textquotesingle " x
%format (QUOTEVAL(x)) = "\textquotesingle " x
%format (PRAGMA(x)) = "\{−\# " x " \#−\}"
%format (FILECHECK(x)) = "−− " x
%format (LHPRAGMA(x)) = "\{−@ " x " @−\}"
%format ghci = "\StyleKW{ghci}"

\renewcommand{\onelinecomment}{\quad–\hspace{.5pt}–\itshape{}}
\renewcommand{\hsindent}[1]{{\quad\strut}}
%subst newline          = "\\[-0.3ex]%'n"
\setlength{\blanklineskip}{0.8ex}
\setlength{\mathindent}{2\parindent}

% new keywords
%format qualified = "\StyleKW{qualified}"
%format as = "\StyleKW{as}"

% highlight this as non-code
%format CInterrupted            = "\StyleAbort{CInterrupted}"

% all modules
%format M                       = "\StyleMod{M}"
%format RS                      = "\StyleMod{RS}"
%format RB                      = "\StyleMod{RB}"
%format RDB                     = "\StyleMod{RDB}"
%format Data.Set                = "\StyleMod{Data.Set}"
%format Data.Set.Internal       = "\StyleMod{Data.Set.Internal}"
%format Data.Map                = "\StyleMod{Data.Map}"
%format Data.Recursive.Set      = "\StyleMod{Data.Recursive.Set}"
%format Data.Recursive.Map      = "\StyleMod{Data.Recursive.Map}"
%format Data.Recursive.Bool     = "\StyleMod{Data.Recursive.Bool}"
%format Data.Recursive.DualBool = "\StyleMod{Data.Recursive.DualBool}"
%format Data.Propagator.P2      = "\StyleMod{Data.Propagator.P2}"
%format Control.Concurrent.MVar = "\StyleMod{Control.Concurrent.MVar}"
%format System.IO.RecThunk      = "\StyleMod{System.IO.RecThunk}"

% all types
%format Exp           = "\StyleType{Exp}"
%format Graph         = "\StyleType{Graph}"
%format Bool          = "\StyleType{Bool}"
%format Map           = "\StyleType{Map}"
%format M.Map         = "\StyleType{M.Map}"
%format Set           = "\StyleType{Set}"
%format S.Set         = "\StyleType{S.Set}"
%format Ord           = "\StyleType{Ord}"
%format RSet          = "\StyleType{RSet}"
%format RS.RSet       = "\StyleType{RS.RSet}"
%format RMap          = "\StyleType{RMap}"
%format V             = "\StyleType{V}"
%format Integer       = "\StyleType{Integer}"
%format String        = "\StyleType{String}"
%format Int           = "\StyleType{Int}"
%format IO            = "\StyleType{IO}"
%format ST            = "\StyleType{ST}"
%format IORef         = "\StyleType{IORef}"
%format Cell          = "\StyleType{Cell}"
%format MVar          = "\StyleType{MVar}"
%format RBool         = "\StyleType{RBool}"
%format RB.RBool      = "\StyleType{RB.RBool}"
%format RDualBool     = "\StyleType{RDualBool}"
%format RDB.RDualBool = "\StyleType{RDB.RDualBool}"

% all constructors
%format Var    = "\StyleConstr{Var}"
%format Throw  = "\StyleConstr{Throw}"
%format Catch  = "\StyleConstr{Catch}"
%format Lam    = "\StyleConstr{Lam}"
%format App    = "\StyleConstr{App}"
%format Let    = "\StyleConstr{Let}"
%format LetRec = "\StyleConstr{LetRec}"
%format True   = "\StyleConstr{True}"
%format False  = "\StyleConstr{False}"
%format MkRSet = "\StyleConstr{MkRSet}"
%format MkCell = "\StyleConstr{MkCell}"

\definecolor[named]{Set3a}{cmyk}{0.60,0.00,0.30,0.00}
\definecolor[named]{Set3b}{cmyk}{0.00,0.45,0.50,0.00}
\definecolor[named]{Set3c}{cmyk}{0.45,0.25,0.00,0.00}

\definecolor[named]{friendlykw}{HTML}{007020}
\definecolor[named]{friendlynamespace}{HTML}{0e84b5}
\definecolor[named]{friendlyconstant}{HTML}{60add5}
\definecolor[named]{friendlyfunction}{HTML}{06287e}
\definecolor[named]{friendlyexception}{HTML}{007020}
\definecolor[named]{friendlyattribute}{HTML}{4070a0}
\definecolor[named]{abortcolor}{HTML}{B80600}

\newcommand{\StyleKW}[1]{\textbf{\textcolor{friendlykw}{#1}}}
\newcommand{\StyleVar}[1]{#1}
\newcommand{\StyleMod}[1]{#1}
\newcommand{\StyleType}[1]{\textcolor{friendlyexception}{#1}}
\newcommand{\StyleCon}[1]{#1}
\newcommand{\StyleConstr}[1]{\textcolor{friendlyfunction}{#1}}
\newcommand{\StyleAbort}[1]{\textbf{\textcolor{abortcolor}{#1}}}

% Ubuntu Light for code
\usepackage{fontspec}
\setmonofont{Ubuntu-R}%
  [ Scale = MatchLowercase
  , BoldFont = Ubuntu-B
  , BoldItalicFont = Ubuntu-BI
  , ItalicFont = Ubuntu-RI
  , WordSpace = {1.5,1,0}
  ]

% URL fix
\makeatletter
% Inspired by http://anti.teamidiot.de/nei/2009/09/latex_url_slash_spacingkerning/
% but slightly less kern and shorter underscore
\let\UrlSpecialsOld\UrlSpecials
\def\UrlSpecials{\UrlSpecialsOld\do\/{\Url@@slash}\do\_{\Url@@underscore}}%
\def\Url@@slash{\@@ifnextchar/{\kern-.07em\mathchar47\kern-.15em}%
   {\kern-.0em\mathchar47\kern-.08em\penalty\UrlBigBreakPenalty}}
\def\Url@@underscore{\nfss@@text{\leavevmode \kern.06em\vbox{\hrule\@@width.3em}}}
\makeatother

\newenvironment{itquote}{\begin{quote}\itshape}{\end{quote}}

\setcopyright{rightsretained}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{XXXXXXX.XXXXXXX}


\acmConference[ICFP'23]{ICFP}{September 04--09,2023}{Seattle, WA}

\citestyle{acmauthoryear}

\begin{document}

\title{Functional Pearl: More fixpoints!}

\author{Joachim Breitner}
\email{mail@@joachim-breitner.de}
\orcid{0000-0003-3753-6821}
\affiliation{%
   \institution{unaffiliated}
%  \institution{Institute for Clarity in Documentation}
%  \streetaddress{P.O. Box 1212}
%  \city{Dublin}
%  \state{Ohio}
  \country{Germany}
%  \postcode{43017-6221}
}


\begin{abstract}
Haskell’s laziness allows the programmer to solve some problems naturally and declaratively via recursive equations. Unfortunately, if the input is “too recursive”, these very elegant idioms can fall into the dreaded black hole, and the programmer has to resort to more pedestrian approaches.

It does not have to be that way: We built variants of common pure data structures (Booleans, sets) where recursive definitions are productive. Internally, the infamous |unsafePerformIO| is at work, but the user only sees a beautiful and pure API, and their pretty recursive idioms -- magically -- work again.

%In the end, this raises interesting questions about the precise nature of purity.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011008.10011024.10011033</concept_id>
       <concept_desc>Software and its engineering~Recursion</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011008.10011009.10011012</concept_id>
       <concept_desc>Software and its engineering~Functional languages</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Recursion}
\ccsdesc[500]{Software and its engineering~Functional languages}

\keywords{Haskell, recursion, fixpoint}

\maketitle

\section{Introduction}

Haskell is a pure and lazy programming language, and this laziness allows us to express some algorithms very elegantly, by recursively referring to currently calculated values. A typical and famous example is the following definition of the Fibonacci numbers as an infinite stream:
\begin{code}
fibs :: [Integer]
fibs = 0 : 1 : zipWith (+) fibs (tail fibs)
\end{code}
This is often called “knot-tying”, because a value (here |fibs|) has a definition involving itself.

\subsection{Tying the knot with graphs}

A maybe more practical example is the following calculation of the reflexive transitive closure of a graph, i.e.\ for each vertex the set of vertices reachable from it. Let us represent a graph as a map from vertices (of type |Int|) to lists of adjacent vertices, which we assume to be in the domain of the map (This keeps the examples concise, as then the lookup operator |M.!| won't fail):
\begin{code}
import qualified Data.Map as M
import qualified Data.Set as S
type Graph = M.Map Int [Int]
\end{code}

The reflexive transitive closure can be very elegantly expressed by knot-tying a map from vertices to their set of reachable vertices:
\begin{code}
rTrans1 :: Graph -> Graph
rTrans1 g = M.map S.toList reaches
  where
    reaches :: M.Map Int (S.Set Int)
    reaches = M.mapWithKey (\v vs -> S.insert v (S.unions [ reaches M.! v' | v' <- vs ])) g
\end{code}
This code is remarkably close to the prosaic specification “the reachable vertices from a vertex |v| are |v| itself, plus all the vertices reachable from any of its successors |vs|”; hence we can consider this program to be \emph{declarative}.

Note how the definition of |reaches| refers to itself -- we are “tying a knot”.

\subsection{It works, until it doesn't}

\begin{figure}
\centering
\hfill
\hfill
\begin{tikzpicture}
\node[draw,circle, inner sep=1pt] (1) at (0,0) {1};
\node[draw,circle, inner sep=1pt] (2) at (1,0) {2};
\node[draw,circle, inner sep=1pt] (3) at (2,0) {3};
\path[draw,->] (1) to [bend left] (3);
\path[draw,->] (2) to [bend left] (1);
\path[draw,->] (2) to [bend right] (3);
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\node[draw,circle, inner sep=1pt] (1) at (0,0) {1};
\node[draw,circle, inner sep=1pt] (2) at (1,0) {2};
\node[draw,circle, inner sep=1pt] (3) at (2,0) {3};
\path[draw,->] (1) to [bend left] (3);
\path[draw,->] (1) to [bend left] (2);
\path[draw,->] (2) to [bend left] (1);
\path[draw,->] (2) to [bend right] (3);
\end{tikzpicture}
\hfill
\hfill
\strut
\caption{Two graphs}\label{fig:graphs}
\end{figure}

This is the kind of code we like to impress our strict-language-using friend with, and it works quite nicely on a small graph (\cref{fig:graphs}, left graph):
\begin{code}
ghci> rTrans1 (M.fromList [(1,[3]),(2,[1,3]),(3,[])])
fromList [(1,[1,3]),(2,[1,2,3]),(3,[3])]
\end{code}
%
At least until our strict-language-using friend challenges us to add just one small edge to the graph (\cref{fig:graphs}, right graph):
\begin{code}
ghci> rTrans1 (M.fromList [(1,[2,3]),(2,[1,3]),(3,[])])
fromList [(1,fromList ^CInterrupted.
\end{code}
Now the graph has a \emph{cycle} ($1 \to 2 \to 1$) which makes our code get lost in an infinite loop, until we abort the program.

This is quite disappointing! In order to handle recursive graphs as input as well, we have to implement this in a much more tedious way, maybe with an explicit loop, keeping track of the set of seen vertices (see \cref{sec:imp} if you really want to see it, but the goal is that you shouldn’t have to). It works, and most of us have likely written that idiom before, but we cannot impress our friend with that.

But it seems it should work: The declarative specification, from which we have derived |rTrans1|, holds for recursive graphs as well, so it does not seem too unreasonable to expect the above code to handle recursive graphs as well. Where does it go wrong? The way we use the lazy map data structure is fine; it helps us to express the set of reachable vertices by way of other such sets.
The problem is that the set data structure, with its operations |insert| and |union|, is not lazy enough: |union| wants to know the value of its arguments before it can produce something useful, and thus cannot be used in a recursive, knot-tying way.

\subsection{We need better sets!}

In this paper we present a data structure for sets, called |RSet|, where such recursive expressions do work! Its API is almost the same as that of |Data.Set|, Haskell's standard library for finite sets. In particular, it also consists of plain \emph{pure} functions -- no monads necessary. The fragment of the API relevant for our example is:
\begin{code}
insert  :: Ord a =>  a -> RSet a  -> RSet a
unions  :: Ord a =>  [RSet a]     -> RSet a
get     ::           RSet a       -> Set a
\end{code}
We find the two operations used by |rTrans1|, with type signatures mirroring those of |Data.Set| exactly, and a function |get| that converts such a |RSet| to a normal |Set|.

We can use this data structure without changing the structure of our code; we just swap out the operations (imported |qualified as RS|) and convert back to conventional sets in the end:
\begin{code}
rTrans2 :: Graph -> Graph
rTrans2 g = M.map (S.toList . RS.get) reaches
  where
    reaches :: M.Map Int (RS.RSet Int)
    reaches = M.mapWithKey (\v vs -> RS.insert v (RS.unions [ reaches M.! v' | v' <- vs ])) g
\end{code}

And indeed, this \emph{can} handle recursive graphs, and get the correct result:
\begin{code}
ghci> rTrans2 (M.fromList [(1,[2,3]),(2,[1,3]),(3,[])])
fromList [(1,[1,2,3]),(2,[1,2,3]),(3,[3])]
\end{code}

\subsection{Contributions}

From the user’s point of view, that is almost all there is  to say: There is a library of types (sets, Booleans, maps) you can use like the conventional types, and suddenly your favorite knot-tying tricks work even better. In \cref{sec:exploration} we'll explore this library from the user's point of view, followed by a larger program analysis case-study in \cref{sec:casestudy}. Then we take a look at how the library works under the hood (\cref{sec:impl}), and point out some limitations (\cref{sec:limitations}). Finally we take a brief glance at related approaches (\cref{sec:related}).

\medskip\noindent

The main contribution of this paper is to demonstrate that we can make more data types recursively defineable, and thus solve more problems elegantly and declaratively.
This can be implemented as a regular Haskell library\footnote{\url{https://hackage.haskell.org/package/rec-def}}, using GHC's |unsafePerformIO| primitive under the hood, but providing a \emph{safe} and \emph{pure} interface.

\iffalse
The main contributions of this paper are:

\begin{itemize}
\item We demonstrate that by making more data types recursively definable, more problems can be solved elegantly and declaratively. They are \emph{safe}, \emph{pure} and are a natural fit for a language like Haskell.

\item We show how to implement this as a regular library, using GHC's |unsafePerformIO| primitive under the hood.\footnote{The library can be found as \texttt{rec-def} on Hackage; an anonymized copy is included in this submission.}

\item We discuss the difficulties of answering the question whether a language extension like this can still be considered \emph{pure}, contributing questions rather than conclusive answers.

%\item observing, as a hopefully enlightening insight, that one of the main features of \emph{laziness} is that it allows more recursive equations to be solved, and thus more problems be expressed declaratively, and thus a the lazier the language, the more declarative.
\end{itemize}
\fi

\begin{acks}
We like to thanks Sebastian Graf and Claudio Russo, as well as the anonymous reviewers, for their helpful comments.
\end{acks}

\section{Exploration}\label{sec:exploration}

In the introduction we have used a data type |RSet| with an API that resembles that of the |Set| data structure in Haskell's |Data.Set| library. Let us explore this data structure some more from the user's point of view, to get a better understanding of how it is different from the ordinary |Set|, and to whet the appetite for the look at its implementation in \cref{sec:impl}.
\Cref{fig:api} gives a comprehensive overview of the API.

\subsection{Just an isomorphic copy?}

\afterpage{\clearpage}
\begin{figure}[p]
\begin{code}
module Data.Recursive.Set where       -- imported as RS here
  data RSet a
  get           ::           RSet a ->            Set a
  mk            ::           Set a ->             RSet a
  empty         ::                                RSet a
  singleton     ::           a ->                 RSet a
  insert        :: Ord a =>  a -> RSet a ->       RSet a
  delete        :: Ord a =>  a -> RSet a ->       RSet a
  union         :: Ord a =>  RSet a -> RSet a ->  RSet a
  unions        :: Ord a =>  [RSet a] ->          RSet a
  intersection  :: Ord a =>  RSet a -> RSet a ->  RSet a
  member        :: Ord a =>  a -> RSet a ->       RBool
  null          ::           RSet a ->            RDualBool
  when          ::           RBool -> RSet a ->   RSet a
  id            ::           RSet a ->            RSet a

module Data.Recursive.Bool where      -- imported as RB here
  data RBool
  get          :: RBool ->           Bool
  mk           :: Bool ->            RBool
  true, false  ::                    RBool
  (&&), (||)   :: RBool -> RBool ->  RBool
  and, or      :: [RBool] ->         RBool
  not          :: RBool ->           RDualBool
  id           :: RBool ->           RBool

module Data.Recursive.DualBool where  -- imported as RDB here
  data RDualBool
  get          ::  RDualBool ->               Bool
  mk           ::  Bool ->                    RDualBool
  true, false  ::                             RDualBool
  (&&), (||)   ::  RDualBool -> RDualBool ->  RDualBool
  and, or      ::  [RDualBool] ->             RDualBool
  not          ::  RDualBool ->               RBool
  id           ::  RDualBool ->               RDualBool
\end{code}
\caption{The API of recursively definable sets and Booleans}\label{fig:api}
\end{figure}

At the first glance, |RSet| looks like a isomorphic copy of |Set|, with |get :: RSet a -> Set a| and |mk :: Set a -> RSet a| converting between the types, and all the operations on |RSet| behave as their counterpart on |Set|. Let's quickly check that \citep{quickcheck}:
\begin{code}
ghci> quickCheck $ \s -> RS.get (RS.mk s) === s
+++ OK, passed 100 tests.
ghci> quickCheck $ \s1 s2 -> RS.get (RS.union s1 s2) === S.union (RS.get s1) (RS.get s2)
+++ OK, passed 100 tests.
\end{code}

The second equation generalizes to all operations in the API, giving them a straight-forward specification in terms of the corresponding operation on the underlying ordinary data type. But there must be a difference, else we would not be writing this paper.

\subsection{Recursion!}

The difference is that with |RSet|, \emph{recursively defined expressions work!} For example, using the ordinary |Set| from |Data.Set| (imported qualified as |S|), evaluating recursive expressions tends to hang:
\begin{code}
ghci> let s = S.insert 42 s in s
fromList ^CInterrupted.
\end{code}
With |RSet|, it simply works:
\begin{code}
ghci> let s = RS.insert 42 s in RS.get s
fromList [42]
\end{code}
It works for larger expressions as well
\begin{code}
ghci> let s = RS.insert 42 (RS.union (RS.insert 23 s) (RS.delete 42 s)) in RS.get s
fromList [23,42]
\end{code}
Not even mutual recursion poses a problem:
\begin{code}
ghci>  let  s1 = RS.insert 42 s2
ghci|       s2 = RS.insert 23 s3
ghci|       s3 = RS.delete 42 s1
ghci|  in (RS.get s1, RS.get s2, RS.get s3)
(fromList [23,42],fromList [23],fromList [23])
\end{code}

In these examples, we build the graph of recursively defined |RSet| values explicitly, using |let|. In practice one would more likely construct that graph using lazy data structures and knot-tying, maybe dynamically based on some input, as done in the introduction.

\subsection{Fixpoints}

It may come as a pleasant surprise that these expressions are productive, i.e.\ that we even obtain a (non-bottom) result. But is it the right result? If we look at the last example above we can see that the ordinary sets we get for each of the three variables makes the three defining equations true:
\begin{code}
ghci> let s1 = S.fromList [23,42]; s2 = S.fromList [23]; s3 = S.fromList [23]
ghci> s1 == S.insert 42 s2
True
ghci> s2 == S.insert 23 s3
True
ghci> s3 == S.delete 42 s1
True
\end{code}

That is good, because that is how we want equations in a functional programming language to behave.

At this point you might interject that these are not the only possible solutions to this system of equations. Returning to the smaller example of |let s = RS.insert 42 s|, we find that our result |S.fromList [42]| indeed solves the equation |s == S.insert 42 s|, but so does |S.fromList [42,43]|. Still, we would not consider that a “good” solution, and would be surprised if we'd get that.
%
That is because we expect the result to be the \textbf{least fixpoint}; the solution that is, among all possible solutions, the smallest with regard to a particular partial order.

In the context of sets the natural order is subset inclusion. Therefore, a possibly recursive expression of |RSet| values will evaluate to the smallest sets solving the definitional equations.

It will always do so, provided that
\begin{itemize}
\item only finitely many |RSet| values are involved and
\item no definition of a |rs :: RSet| depends on the expression |RS.get rs|.
\end{itemize}
Using |RS.get| drops us in the world of ordinary sets, and the magic disappears:
\begin{code}
ghci> let s = RS.mk (RS.get (RS.insert 42 s)) in RS.get s
fromList ^CInterrupted.
\end{code}
In this sense, |RS.mk . RS.get| is not the identity function.

\subsection{More than sets}

The library not only provides recursively definiable sets, but also other data types, in particular a variant of the ordinary Booleans that are recursively definiable; see \cref{fig:api} for an excerpt of the API. Again, we have analogues of the usual operations (literals, conjunction and disjunction), and as before, a possibly recursively defined expression of type |RBool| will evaluate to “the” Boolean value which solves these defining equations.

What happens if both |True| and |False| would solve an equation, like in the following case?
\begin{code}
ghci> let x = x RB.|| x in RB.get x
False
\end{code}
We can see that |RBool| considers |False| as the least element, and for some use-cases that is the right choice. But for other use-cases, one would prefer |True| over |False|. Therefore, the library provides a separate module and data type |RDualBool|, again with the full set of operations on Booleans, but this time returning |True| if possible:
\begin{code}
ghci> let x = x RDB.|| x in RDB.get x
True
\end{code}

\subsection{Monotonicity}\label{sec:monotonicity}

These data types -- |RSet|, |RBool|, |RDualBool| -- are not silos, and you will find among the functions in \cref{fig:api} some that connect these types: negation on Booleans, member checks on sets, the emptiness check on sets, and the function |when|, which guards a set depending on a boolean.
This means that even recursive expressions involving multiple of these types will produce a result.

So why does |RS.member| return a |RBool|, but |null| returns a |RDualBool|? And why is there no function |not :: RBool -> RBool|? It is because all functions involved here must be \emph{monotonic}: smaller arguments must lead to smaller results. And because the empty set is smaller than non-empty sets, |RS.member| must return a |RBool| (where |False| is smaller than |True|), but |null| must go to |RDualBool| (where |True| is smaller than |False|).

If we did not pay attention to this while defining the API, and added non-monotonic functions (like |not :: RBool -> RBool|), the user would be  able to write equations that do not have a solution, such as
|let x = not x|,
and we would like to statically rule that out.

The underlying bit of theory is the theorem that a monotone function $f : A \to A$ on a partially ordered set $A$ with least element $\bot \in A$ and finite height has a unique least fixed point. This is well-known (e.g.\ see \citep{lazyleast}), and if the sentence means something to you, you probably already saw it coming. And if it doesn't, it does not matter for reading this paper.

This also explains why some functions from the underlying ordinary data type (such as |Data.Set|'s |difference|) are not available, as they are not monotonic.

\subsection{Termination}

Another function from |Data.Set| that we do not have in |Data.Recursive.Set| is the function |map :: Ord b => (a -> b) -> Set a -> Set b|. This may be a bit surprising, as this function is perfectly monotonic with regard to the subset relation. But it can cause other problems: Imagine we had it, and wrote
\begin{code}
let s = RS.insert 0 (RS.map (+1) s) in RS.get s
\end{code}
Does this equation have a solution? Clearly the set |s| needs to contain |0|. But then it also needs to contain |1|. And |2|. And so on. So the solution would have to be the set of \emph{all} natural numbers, but that is not something that |Data.Set|, being a data structure of \emph{finite} sets, can represent.

So we cannot allow this function for |RSet| if we want to guarantee a result for every finite recursive expression.

For the theoretically inclined, this plays into the “$A$ has finite height” requirement in the theorem above. You might be irked that the type |Set a|, ordered by subset inclusion, does not actually have finite height (if |a| is not finite). With the current API (without functions like |map|) for every \emph{finite} |RSet| expression there are only finitely many possible members, and thus the relevant “subtype” has finite height, and all is well again.

It would not be unreasonable, however, to add |map| to the |RSet| API, as it may be quite useful for some applications, and maybe in these applications the equation have a finite solution just fine. If we'd do that, we could no longer \emph{guarantee} termination for all possible expressions (as shown by the example above), but if expression yields a result, it will be the least fixed point of the defining equation. One can argue that this would be fine for a Haskell library, as Haskell programmers are used to programming around non-termination anyways.

\subsection{The black hole}\label{sec:blackhole}

We said that “all finite, possibly recursive expressions yield a result”. Unfortunately, that is not completely true: If a value of type |RSet| is defined to be simply itself, with none of the |RSet| operations involved, it will not work:
\begin{code}
ghci> let s = s in RS.get s
fromList ^CInterrupted.
\end{code}
And it’s not for lack of a solution: Clearly the empty set is the least solution to the equation |s == s|.

Because our library is but a library, despite the apparent magic inside (which we will uncover in \cref{sec:impl}), with a definition like |let s = s|, it has no chance to insert its magic.

The problem goes away as soon as \emph{any} function from the API is involved in the definition, even if it is semantically the identity:
\begin{code}
ghci> let x = RS.unions [x] in RS.get x
fromList []
\end{code}

This is a little stumbling block when using this library. And while programmers are unlikely to write |let x = x| directly, the same thing can happen when tying the knot via a lazy data structure. In that case, the programmer is advised to insert a semantic identity function in a suitable position; the API provides |RS.id :: Set a -> Set a| for that purpose.
A programming language that integrates these features first-class could take care of this automatically.

\section{Case study: a program analysis}\label{sec:casestudy}

Before we leave the user's point of view and look under the hood of the library, let us walk through a slightly larger and more realistic use-case. We hope that this example shows that using recursively definable values allows for noticeably more declarative and elegant programs.

\subsection{First without recursion}

Let us consider small program analysis of a functional language with lazy let-bindings, (mutual) recursion and exceptions. It should determine whether evaluating an expression may throw an (uncaught) expression.

To set the stage, \cref{fig:analast} contains a typical Haskell datatype for its abstract syntax. The |LetRec| constructor takes a list of declarations (each with a name and definition) and a body; all bound variables are in scope in all the right-hand sides and the body.
Variable names bound in |Lam|, |Let| and |LetRec| shadow outer occurrences. (The resemblance to GHC's intermediate language Core \citep{secrets} is certainly not a coincidence.)

\begin{figure}
\setlength{\abovedisplayskip}{0pt}%
\setlength{\belowdisplayskip}{0pt}%
\begin{code}
type V    =  String
data Exp  =  Var V | Lam V Exp | App Exp Exp | Throw | Catch Exp
          |  Let V Exp Exp | LetRec [(V, Exp)] Exp
\end{code}
\caption{An AST for a functional language with mutual recursion and exceptions}
\label{fig:analast}
\end{figure}

Let us ignore |LetRec| at first, and write our analysis as a simple traversal of the AST:
\nopagebreak
\savecolumns
\begin{code}
canThrow :: Exp -> Bool
canThrow e = go M.empty e
  where
    go :: M.Map V Bool -> Exp -> Bool
    go env (Var v)        = env M.! v
    go env Throw          = True
    go env (Catch e)      = False
    go env (Lam v e)      = go (M.insert v False env) e
    go env (App e1 e2)    = go env e1 || go env e2
    go env (Let v e1 e2)  = go env' e2
      where
        env_bind  = M.singleton v (go env e1)
        env'      = M.union env_bind env
\end{code}

Our language is lazy, so to determine whether evaluating a variable can throw,
we have to carry around an
environment of type |M.Map V Bool| where we remember whether the corresponding right-hand side could throw. |Throw| and |Catch| certainly can resp.\ cannot throw. The analysis isn't higher order, so for lambdas we assume they can throw if their body can throw, and for applications if either subexpression can throw. Finally for |Let|, we extend the environment with the analysis result of the bound variable's right-hand side and descend.

\subsection{Avoiding the black hole}
So far, so standard. But what about |LetRec|? Here, the analysis result of each right-hand side depends on the analysis results of all right-hand sides. We can try to simply do what we did in the |Let| case:
\restorecolumns
\begin{code}
    go env (LetRec binds e) = go env' e
      where
        env_bind  = M.fromList [ (v, go env' e) | (v,e) <- binds ]
        env'      = M.union env_bind env
\end{code}
Note that, crucially, we use the extended environment |env'| not only for the body, but also for the right-hand sides.

Alas, this does not work: As soon as we try to analyze an expression that uses recursion, we will fall into a black hole. The crux is that |Bool| is not recursively definable, because its operations (here only disjunction, |(||||)|) are not lazy.

But if we use |RBool| instead of |Bool|, it just works:
\restorecolumns
\begin{code}
canThrow :: Exp -> Bool
canThrow e = RB.get (go M.empty e)
  where
    go :: M.Map V RBool -> Exp -> RBool
    go env (Var v)          = env M.! v
    go env Throw            = RB.true
    go env (Catch e)        = RB.false
    go env (Lam v e)        = go (M.insert v RB.false env) e
    go env (App e1 e2)      = go env e1 RB.|| go env e2
    go env (Let v e1 e2)    = go env' e2
      where
        env_bind  = M.singleton v (go env e1)
        env'      = M.union env_bind env
    go env (LetRec binds e) = go env' e
      where
        env_bind  = M.fromList [ (v, RB.id (go env' e)) | (v,e) <- binds ]
        env'      = M.union env_bind env
\end{code}
All we had to do was to use |RBool| instead of |Bool| in the type of the local, use the corresponding operations (|RB.true|, |RB.false| and |RB.|||||) and  project out to normal Booleans at the end (using |RB.get|). A slight blemish is that we also had to insert a call to |RB.id| to not fall over the input |LetRec [("x", Var "x")]|, as explained in \cref{sec:blackhole}.

\subsection{Using all the values}

The example above calculates only one final result, whether the whole expression canThrow. A real compiler pass might want to use the analysis result at each node to update the AST (e.g.\ remove redundant calls to |Catch|, or remember analysis results in the AST as GHC would do). To keep the example simple for the paper we do not do that, as the necessary plumbing would obscure the point.

But our library does allow it: we can use |RB.get| within the function to get the analysis result for \emph{any} subexpression and return a changed AST accordingly, e.g.
\restorecolumns
\begin{code}
    go :: M.Map V RBool -> Exp -> (RBool, Exp)
    ...
    go env (Catch e)        = (RB.false, new_e)
      where
        (can_throw, e') = go env e
        new_e  |  RB.get can_throw  = Catch e'
               |  otherwise         = e'
\end{code}
We'd have to be careful that the |RBool| returned by |go| does not depend on any decision made based on a boolean that was obtained with |RB.get|. For such a fused analysis/transformation pass this is typically possible.

%The ability to query each “vertex” as we go sets this approach apart from other idioms with a

\subsection{Alternatives}
What would we do if we did not have |RBool| at our disposal? Here are some common options:
\begin{itemize}
\item We can perform an explicit fixpoint analysis in the |LetRec| case: Initialize the |env_bind| with all variables mapped to |False|, descend, check if now any analysis result has changed, and if so, re-analyze all of them, until we found a solution.

Maybe we can be more clever and re-analyze only some of them.

Maybe some of that logic can be extracted into a suitable fixpoint operator.

In any case, we would obscure the declarative intent of the code with lower-level bookkeeping.

\item If we do that naively, we can run into the problem that in the presence of nested recursive lets, the nested fixpoint iteration comes with exponential complexity.

In the case where the analysis result is persisted as annotations in the syntax tree anyways, we can address this issue by starting the fixpoint iteration not from bottom, but from the result of the previous outer iteration.
%
GHC does this, as explained in \citet[Section 6.6]{modular}, but again at the cost of more plumbing obscuring the code's intent.

\item Another approach is to gather the full data flow problem from the \emph{whole} AST, solve it globally (thus also avoiding the problem mentioned in the previous bullet), and then distributing the analysis again to where we need it. This is reminiscent of how a constrained-based type inference algorithm works.

It is a satisfying thought that this approach frees the solving algorithm from having to follow the syntactic nesting structure of the program, and that the repeated passes of the fixpoint iteration do not have to traverse and re-analyze the AST over and over. Instead it only sees the pure, distilled data-flow equations.

There are, however, petty practical issues with this approach (representing the equations as data, uniquely naming the cells, the separate passes for collecting the equations and using the results).
At this point, one is likely going to hide this bookkeeping in a suitable monad, which can recover some of the lost elegance, but if the code could otherwise be written as pure functions, that is still quite a price to pay.%
\footnote{See \url{https://hackage.haskell.org/package/datafix-0.0.1.0/docs/Datafix-Denotational.html\#v:datafix} for an attempt.}
\end{itemize}

When our library is applicable, it allows us to retain the concise elegance of the pure code that does not bother with the “how” of solving equations, while under the hood the solver has a comprehensive global view of the problem.


\section{Under the hood}\label{sec:impl}

We hope that by now you are eager to learn how the |RSet| library is implemented. It is a regular Haskell library, without dedicated compiler support nor using compiler plugins. Maybe this sounds impossible, and we agree: The API and specification presented in the previous section \emph{cannot} be implemented in normal, safe, pure Haskell code.\footnote{At least we believe it is not possible, as we explain in \cref{sec:lim-lambda-lift}.}

But it can be implemented using \emph{“unsafe”} features; in particular GHC's infamous function |unsafePerformIO :: IO a -> a|, which allows arbitrary side-effects in pure code. Before you turn away in disgust please allow us to quote \citet{unsafePerformIO} from their publication introducing this primitive:
\begin{quote}
However “unsafe” is not the same as “wrong”. It simply means that the programmer, not the compiler, must undertake the proof obligation that the program's semantics is unaffected by the moment at which all these side effects take place. [\ldots]
So, we regard the primitives of this paper as \emph{the raw material from which experienced systems programmers can construct beautiful abstractions}.
\end{quote}
This is our goal; whether the abstraction presented in \cref{sec:exploration} is beautiful is in the eye of the beholder.

\subsection{A naive implementation}

The core idea behind the implementation can be explained in two simple steps: First, we use an imperative API to declare values, register their relationships and read their values, and then we wrap that in a pure and sufficiently lazy API. We begin by outlining a naive implementation that initially ignores issues of reentrancy-safety, modularity, performance and space-leaks.

\subsection{An imperative core}\label{sec:impcore}

A typical \emph{imperative} API to describe and solve a set of recursive equations provides functions to
\begin{enumerate}
\item register the variables, or \emph{cells},that occur in the equations
\item define how such a cell is related to other cells
\item finally read the value of the cells.
\end{enumerate}
To keep the example code small, we focus on just sets and insertion as the only operation, and could imagine an API like the following:

\begin{minipage}{\linewidth}
\begin{code}
data Cell a
newCell        ::                                      IO (Cell a)
defCellInsert  ::  Ord a =>  Cell a -> a -> Cell a ->  IO ()
getCell        ::            Cell a ->                 IO (Set a)
\end{code}
\end{minipage}

A typical use of this API, solving the two mutually related set equations
\begin{align*}
s_1 &= \{ 42 \} \cup s_2 \\
s_2 &= \{ 23 \} \cup s_1
\end{align*}
would be
\begin{code}
ghci> c1 <- newCell
ghci> c2 <- newCell
ghci> defCellInsert c1 42 c2
ghci> defCellInsert c2 23 c1
ghci> getCell c2
fromList [23,42]
\end{code}

At this point, the actual implementation of this API is not that interesting. In a simple implementation a cell would consist of a current value (initialized to the empty set), and a list of cells depending on this value, and then the changes due to calls to |defCellInsert| are propagated through this network until no more changes need to be propagated (\cref{sec:impcoreimpl} re-phrases this summary in Haskell). Of course, more sophisticated algorithms may lurk underneath this interface.

\subsection{The pure wrapping}

The more interesting question is how to get from the imperative |Cell| code to the pure |RSet| API, consisting of just |insert :: a -> RSet a -> RSet a| and |get :: RSet a -> Set a|?

Clearly, |insert| must somehow both create a new cell, and define its equation.
Furthermore, it has to be lazy in its second argument, else a recursive equation would immediately loop, so it somehow has to defer the call to |defCellInsert| a bit. This leads to the code seen in \cref{fig:wrap}, which we go through in detail:

A value of type |RSet| consists of three fields
\begin{itemize}
\item The |Cell a| backing the value we are defining.
\item An |IO ()| action, deferred until the value is actually needed. A value is needed if |get| is used on the |RSet|, or on another |RSet| that depends on this one.
\item A flag to remember if this deferred action has run already.
\end{itemize}


\begin{figure}%
\setlength{\abovedisplayskip}{0pt}%
\setlength{\belowdisplayskip}{0pt}%
\raggedright%
\begin{code}
data RSet a = MkRSet (Cell a) (IO ()) (IORef Bool)

insert :: Ord a => a -> RSet a -> RSet a
insert x r2 = unsafePerformIO $ do
   c1 <- newCell
   done <- newIORef False
   let  todo = do
             is_done <- readIORef done
             unless is_done $ do
                 writeIORef done True
                 let (MkRSet c2 todo2 _) = r2
                 defCellInsert c1 x c2
                 todo2
   return (MkRSet c1 todo done)

get :: RSet a -> Set a
get (MkRSet c todo _) = unsafePerformIO $ do
   todo
   getCell c
\end{code}%
\caption{Wrapping an imperative propagator library in a pure way}\label{fig:wrap}
\end{figure}

The function |get| does not do much: It triggers the |todo| action, and afterwards returns the current value of the cell. The interesting bits are in the |insert| function: It creates a new cell to represent the result, and a “done”-flag. It returns these together with a |todo|-action, which is \emph{not yet run}. Note that the second argument, |r2|, is \emph{not} looked at yet, so |insert| is lazy, as required.

The |todo| action itself uses the flag to ensure it is only run once. Only therein the value |r2| is analyzed, and the relationship between the cells is registered. It also runs the |todo| action of the other cell. This way, a single call to |get| will recursively trigger the |todo| actions of all involved values -- and the flags prevent that process from running in cycles.

\subsection{Less naively, please}

This code describes the essence of our idea, and easily generalizes to the other operations of the |RSet| API. It is, however, naive in a few ways that are worth discussing.

\subsubsection{Reentrancy and thread safety.}\label{sec:thread}

The |done| flag is used to ensure that the |todo| action is run exactly once. But if |get| is invoked concurrently, the code above is obviously racy.

Even worse: Because this is run from |unsafePerformIO|, even in a single-threaded environment we have to worry about reentrancy, as forcing any unevaluated user-provided expression could kick off execution of another call to |get|.

In our library we address this with careful use of the |MVar| concurrency primitive \cite{concurrent}, and hide this cleanly behind a small abstraction for “possibly recursive |IO|-thunks” in the |System.IO.RecThunk| API.

The dejafu test library \citep{dejafu}, which can exhaustively explore all possible interleavings of concurrent code, has proven invaluable when implementing that abstraction: more than once we thought we had finally achieved thread-safety, only to be told that we were not there yet, until we eventually were able make the tests pass.

\subsubsection{Modularity.}

The naive code above supports just one value type, |Set|, because the underlying imperative propagator library |Cell| only supports that type. The full library abstracts over the underlying propagator. This way we can have recursively defined values of different types (|RSet a|, |RBool|), operations connecting them (e.g.\ |member|) and even allow solving heterogeneously typed sets of equations.

Supporting different propagator libraries also opens the way for important performance optimizations that are specific to various value types. The most generic propagator implementation assumes no structure on its values besides equality, and just keeps propagating changes until the graph has stabilized. But if we can have different propagator implementations for different types, smarter propagator libraries can be written.

For example for Booleans, a cell changes its value at most once, from |False| to |True|. Once it is |True|, it will never change again, and one can drop its connection to other cells (see the |Data.Propagator.P2| module).

Similarly, a propagator library for finite sets can propagate just deltas, instead of always recomputing the sets from its full inputs, to avoid repeating work. (This is not yet implemented in our library, but would be possible without affecting the public API.)

\subsubsection{Space leaks}\label{sec:spaceleak}

Another, maybe subtle, problem with our pure wrapping of an imperative propagator library is that it can easily lead to space leaks.

Consider the |insert| function. With |rs2 = RS.insert x rs1| we create a new mutable cell for |rs2|, and tell the mutable cell in |rs1| to notify the other cell of any changes. This means that now somewhere in |rs1| there is a reference to |rs2| -- exactly the other way around from what one would expect from the code. This reference prevents resources allocated for |rs2| from being freed while |rs1| is alive.

This causes space leaks, where allocated resources are kept alive longer than expected or needed. As an extreme example, consider this interaction
\begin{code}
ghci> RS.get (RS.insert 42 RS.empty)
fromList [42]
\end{code}
The call to |insert| allocates a new cell and registers it with the cell in |RS.empty|. But |RS.empty| is a \emph{static} value, and will never be garbage collected!

To fix this, we exploit that once we are done calculating a value (either because we query it using |get|, or because it is needed in the calculation of such a value), it will never change any further. So our propagator API allows “freezing“ a cell, which drops the references to the dependent cells, and the pure wrapper freezes cells after the value is computed.

\section{Caveats}\label{sec:limitations}

The library presents itself with a innocent looking, pure and simple interface (\cref{fig:api}), but the implementation is full of side-effects (as seen in \cref{sec:impl}). One might justifiably worry: Is this really pure? Did we create a beautiful abstraction, or are the types a lie?

We believe the library is indeed pure: The exposed API is designed so that a set of equations has a \emph{unique} solution, and since it solves these equations, the nice equational theory one expects from a pure functional program still holds. Yet, there are some caveats worth pointing out.

\subsection{Contextual equivalences and lambda lifting}\label{sec:lim-lambda-lift}

In general in Haskell, if we have a recursive value definition involving an expression |e|, it is ok to turn that into a recursive function definition abstracting over that expression: We can go from
\begin{code}
let x = ... x ... e ... in x
\end{code}
to
\begin{code}
let x y = ... (x y) ... y ... in x e
\end{code}
without changing the program's meaning. With our library, this change can now make the difference between termination and non-termination:
\begin{code}
ghci> let rs = RS.insert 42 rs in RS.get rs
fromList [42]
ghci> let rs y = RS.insert y (rs y) in RS.get (rs 42)
fromList ^CInterrupted.
\end{code}

This is not a simple infelicity of the current implementation, but a fundamental limitation: In the original program, our library can get its (magic, |unsafePerformIO|-powered) hands on a \emph{finite} graph of recursively defined values. In the transformed program there is now a recursive function that endlessly creates \emph{new} values, and our library will never see a complete set of equations. It seems that no (reasonable) implementation of the interface in \cref{fig:api} can solve this problem. (This also shows that a pure implementation of that interface indeed does not exist.)

This limitation exists already in Haskell, albeit in a less severe form: The nice and fast knot-tied |fibs| from the introduction becomes horribly inefficient once you turn it into a recursive \emph{function} (|let fibs x y = x : y  : zipWith (+) (fibs x y) (tail (fibs x y))|).

In that sense, lambda-lifting is only an equivalence as long as we ignore (asymptotic) complexity, and breaking that equivalence may not be as bad as it first seems.
In particular, since an optimizing compiler tends to be careful to not worsen the asymptotic complexity, and will not just willy-nilly break sharing, it will not apply transformations that break in the presence of our library.

\Citet[Fact 3.7]{sabry} points out that merely changing the set of observational equivalences does \emph{not}, in general and on its own, imply that the extended language is no longer pure.

\subsection{Equivalence of sets}

The purity of this library rests on the fact that the equations have a unique soluiton, and therefore it does not matter how the result is calculated. This argument has a weakness in the case of |RSet| because the underlying |Set| type, implemented as a weight-balanced binary search tree \citep{adams-tr,nievergelt}, does not have a unique representation, and a different evaluation order may lead to results that are differntly represented. This is only observable by intentionally breaking the abstraction via the |Data.Set.Internal| module, but it is possible.

\subsection{Unproven claims}

Our claims above about the safety and purity of the exposed interface are not yet backed by a formal proof. Executing such a proof would first require a formal notion of what “pure” even means, which is not quite clear. Formal criteria have been proposed by \citet{sabry} and \citet{longley}, but they do not easily apply to our setting. Additionaly, we’d need a formal framework that allows us to reason about lazy programs with |unsafePerformIO|, |MVar|, laziness and concurrency.

The maybe most similar work are the two mechanized proofs that |runST :: ST a -> a| is safe \citep{runST,runST2}: The contextual equivalences of the pure language still hold when extending the language with |runST|, and thus the extension is fully abstract.  However, their languages are not call-by-need (which is crucial in our case) and not concurrent (which is also relevant), and as discussed in the previous section, our extension preserves some, but not all contextual equivalences.

\section{Related work}\label{sec:related}

The present work draws inspiration from and connects to various directions.

\subsection{Fixpoint solvers and propagation networks}

\Cref{sec:impcore} describes the “imperative core” of our library: An underlying library that allows us to declare the cells of a possibly cyclic graph of values with their relations and finds the solution. For the purposes of this paper we can assume this to be a black box, to avoid getting distracted by questions related to the theory and implementation of this black box, which is good, as there is much interesting than can be said and done here.

All the work in making fixpoint solvers and data-flow analyses efficient is relevant here \citep{kildall-73,kam-ullman-76}. An important difference to typical data-flow analyses is that in order to stay pure, we cannot afford to make conservative approximations while solving the equations (e.g.\ aborting with a safe guess after a certain number of iterations).

Also related is the concept of \emph{propagator network} \citep{propagator}, although our use-case is a bit simpler, as information only flows in one direction along an edge, and once we queried the value of one vertex, we never add more constraints that would influence that vertex.

%\subsection{LVars}
%
% One might wonder if Lindsey Kuper's LVar library can be used to implement

\subsection{Shallow graph embeddings and observable sharing}

\Citet{observable-sharing} made sharing observable in Haskell, so that they can very elegantly describe logic circuits in Haskell. They extend pure Haskell with operations that one might dismiss as impure (observable object identity), point out that this breaks some equational equivalences of Haskell, but that this may be acceptable, given that some of these equivalences  are not benign code transformations to begin with, as they can duplicate arbitrary amount of work anyways -- a line of reasoning you may recognize from \cref{sec:lim-lambda-lift}.

Their extension to Haskell is bolder: It breaks more laws (even referential transparency!) in order to make the structure of the embedded graph observable, as necessary for their poster use-case. With our library, you \emph{cannot} observe the structure of the graph; you only get your hands on the unique solution to the equations. This means it is unsuitable for some use-cases, but also less disruptive.

We could have implemented our library on top of their observable sharing mechanism, reifying the graph of values as a data structure with explicitly named vertices. We chose to do it differently: Haskell's laziness and the guarantee that an application |x = unsafePerformIO act| is executed at most once makes sharing of the value |x| observable. For each shared value we can thus create a unique mutable cell, and let them refer to each other.
The graph is never actually turned into a (Haskell-level) data structure and the cells are not (explicitly) numbered or named; instead the graph is represented by the pointers on the heap, cells are implicitly identified by their object identity, and the bookkeeping data for each cell is stored within its mutable fields.

Looking beyond Haskell we want to point out CoCaml \citep{cocaml}, which allows the Ocaml programmer to observe the structure of knot-tied (coinductive) data, and even process such data while preserving the sharing (so that a |map| over a cyclic list can return a cyclic list).


\subsection{Logic programming}

For the kind of use-cases presented here, logic programming languages like Prolog and Datalog are certainly on their home turf. What we bring to the table is the seamless integration into an existing purely functional language.

\subsubsection{Datafun}

Particularly close to our work, and straddling the functional and logic programming paradigms, is the \emph{Datafun} language \citep{datafun}, a pure and total functional programming language generalizing Datalog, which can declaratively express and compute fixed points of monotone maps on semilattices -- exactly what we are trying to do. Since they tailor their language around this idea, their type system can recognize \emph{monotonic} function definitions.

\newcommand{\mfun}{\xrightarrow{\raisebox{-3pt}[0pt][0pt]{+}}}

Monotonicity is crucial for the existence and uniqueness of a solution (\cref{sec:monotonicity}). We ensure monotonicity by simply restricting the interface (\cref{fig:api}) to only expose monotonic functions. This works, but is rather limiting when it comes to higher-order functions. Assume we would want to support recursively-definable finite \emph{maps} as well. A natural order on finite maps is the point-wise ordering. But with that ordering, higher-order operations like \mbox{|map :: (a -> b) -> RMap k a -> RMap k b|} are only monotonic if their argument is monotonic, and Haskell's type system does not allow us to express that constraint. Without these higher-order functions, however, the map API would be quite impoverished, so we do not support finite maps with the point-wise ordering in our library.\footnote{You might notice the |Data.Recursive.Map| module in the |rec-def| package. This implements maps with the \emph{discrete} ordering on values, where all higher order function arguments are vacuously monotonic, thus avoiding this issue.} Datafun's type system has a separate function arrow $\mfun$ to characterize monotonic functions, and thus supports this use-case quite well.

\subsubsection{Hatafun}

Could we have such such a function arrow $\mfun$ in Haskell? This idea is explored by \emph{Hatafun} \citep{hatafun}, an embedding of Datafun into Haskell. It defines |newtype a -+> b = MFun (a -> b)| and its API, if used correctly, only allows monotonic functions.

\section{Conclusion and further work}

We saw that we can extend Haskell with the ability to solve recursive equations involving Boolean and sets, that this extension can be implemented as a library, and that this extension -- arguably -- preserves the nice properties of the language.

Of course there is more to be done. On the practical side there are more data types and operations to be included in the library (natural numbers in various ordering, maps). On the theoretical side, a more rigorous, formal approach to the question of whether this is actually safe and pure is worth pursuing, as is finding a denotational way to describe what the library do.

\bibliographystyle{ACM-Reference-Format}
\bibliography{bib}

\clearpage

\appendix
\section{The reflexive transitive closure, pedestrian-style}\label{sec:imp}

In the introduction we mentioned that without the data structure presented in this paper, a programmer likely has to write their reflexive-transitive-closure code with an explicit loop (a tail-recursive |go| function), explicitly keeping track of seen vertices to avoid running in circles:
\begin{code}
rTrans3 :: Graph -> Graph
rTrans3 g = M.fromList [ (v, S.toList (go S.empty [v])) | v <- M.keys g ]
  where
    go :: S.Set Int -> [Int] -> S.Set Int
    go seen [] = seen
    go seen (v:vs)  |  v `S.member` seen  =  go seen vs
                    |  otherwise          =  go (S.insert v seen) (g M.! v ++ vs)
\end{code}

\section{The imperative core's implementation}\label{sec:impcoreimpl}

In \cref{sec:impcore} we assumed a module exporting the following API:
\begin{code}
data Cell a
newCell        ::                                      IO (Cell a)
defCellInsert  ::  Ord a =>  Cell a -> a -> Cell a ->  IO ()
getCell        ::            Cell a ->                 IO (Set a)
\end{code}

\noindent 
A simple implementation could look like this:

\typeout{(./code/Cell.lhs)} % For latexmk
%include ./code/Cell.lhs


\ifpure
\section{Is this still Haskell?}\label{sec:pure}

The library presents itself with a innocent looking, pure and simple interface (\cref{fig:api}), but the implementation is full of side-effects (as seen in \cref{sec:impl}).
Does this leak, or does programming with it still feel like programming Haskell?
Is this really pure? Did we create a beautiful abstraction, or are the types a lie?

In this section we discuss these questions, however without always being able to give a definite answer, and invite you to join the discussion.

\subsection{What does purity even mean?}

We found that this question is not easy to answer, because there does not exist a single, clear and widely accepted definition of “pure” functional programming.
This paper is not the place to give an authoritative answer, so we look at various language properties commonly associated with a pure functional language in general and Haskell in particular,
and see whether they still hold in the presence of our library.

\subsection{Type safety}

Purity probably means little without memory and type safety, and we certainly want to preserve Haskell's safety properties: So what we get out of |RS.get| should really be a valid |Set|, and not some corrupted memory.

It is not hard to break type safety with |unsafePerformIO|, especially when there are mutable references and polymorphism around. Nevertheless, we are fairly confident that we did not and that our library is type and memory safe.

\subsection{No (observable) side effects}

Our library uses |IO| inside, and clearly we would not consider it to be pure if inserting |42| into a set would make the computer eject the CD from its drive. Avoiding such blatant side effects is simple -- we just don't do it.

However, |RS.insert 42 rs| \emph{does} destructively modify mutable data structures inside |rs|. It is crucial that these changes remain internal to the library, and are not observable by the programmer. It must not affect the result the programmer gets via |RS.get rs|, nor any other \emph{observable} value. Again, we believe that this is the case, for the same reasons we believe we maintain referential transparency (\cref{sec:reftrans}).

Note that the space leak issue of the naive implementation described in \cref{sec:spaceleak} is \emph{almost} an issue of this kind: If we’d include space leaks in our notion of “observable behavior”, then that would be an example of such a side-effect issue.

\subsection{Evaluation order independence}

Even more, it should not matter in which order the various subexpressions are evaluated. In
\begin{code}
ghci>  let  s1 = RS.insert 42 s2
ghci|       s2 = RS.insert 23 s3
ghci|       s3 = RS.delete 42 s1
ghci|  in (RS.get s1, RS.get s2, RS.get s3)
\end{code}
we need to get the same result, no matter whether we evaluate the first component of the resulting tuple before the second, or the other way around, or even all in parallel in separate threads.

Achieving this in our library required some care. Because our computation are triggered via |unsafePerformIO|, we have to expect any of them to happen at almost any time. Simple graph traversal methods, which simply mark a vertex as “done” before visiting its successors, easily go wrong when a second evaluation hits such a supposedly “done” vertex while the first traversal is still processing the graph. Getting this to the point where it seems to be working was non-trivial, as mentioned in \cref{sec:thread}.

In the case of |RSet| this guarantee may no longer hold if one breaks the abstraction of the |Data.Set| library and looks at the internal represenation, as the same set can be represented by differently balanced search trees internally.

\subsection{Referential transparency}\label{sec:reftrans}

Purity is usually understood to also imply referential transparency: If |x| is defined to be |e|, then we can replace an occurrence |x| with |e|, and vice-versa, without changing the meaning of the program.
(Care has to be taken when applying this to recursive definitions. Of course we do not expect |let x = 1 in x| to be the same as |let x = x in x|, although we merely replaced |1| with |x|.)

So is our library still referential transparent? By virtue of always calculating a unique fixpoint, it is!  The value of |let rs2 = RS.insert 42 rs1| is the (least) set for which the equation |RS.get rs2 == S.insert 42 (RS.get rs1)| is true, and thus |rs2| and |RS.insert 42 rs1| can be used interchangeably.

This restriction to fixpoints, i.e.\ to solutions of a set of equations, puts a clear limit on the kind of observations we allow the programmer to make about the recursive equations. For example, it would not be pure if the programmer could obtain the number of equations. This rules out using our approach for use-cases where the actual \emph{structure} of the equations (and not just a solution) is of interest, e.g.\ when generating circuit descriptions \cite{observable-sharing}.

\subsection{Program equivalences and lambda lifting}\label{sec:sat}

Referential transparency is a crucial ingredient to equational reasoning: rewriting the code according to a set of rules, while preserving its meaning. Beyond just replacing |x| with its definition |e| (also called \emph{unfolding} or \textdelta-reduction) there are further program transformations that we expect to preserve semantics. They may be applied by optimizing compiler, used in program verification or simply by a programmer trying to understand the meaning or improve the structure of some code.

Ideally, all program transformations that are meaning-preserving in pure Haskell would still meaning-preserving in the presence of our library. Unfortunately, this is not the case: Lambda lifting can turn terminating programs into non-terminating ones.

In general in Haskell, if we have a recursive value definition involving an expression |e|, it is ok to turn that into a recursive function definition abstracting over that expression: We can go from
\begin{code}
let x = ... x ... e ... in x
\end{code}
to
\begin{code}
let x y = ... (x y) ... y ... in x e
\end{code}
without changing the program's meaning. With our library, this can now make the difference between termination and non-termination:
\begin{code}
ghci> let rs = RS.insert 42 rs in RS.get rs
fromList [42]
ghci> let rs y = RS.insert y (rs y) in RS.get (rs 42)
fromList ^CInterrupted.
\end{code}

This is not a simple infelicity of the current implementation, but a fundamental limitation: In the original program, our library can get its (magic, |unsafePerformIO|-powered) hands on a \emph{finite} graph of recursively defined values. In the transformed program there is now a recursive function that endlessly creates \emph{new} values, and our library will never see a complete set of equations. It seems that no (reasonable) implementation of the interface in \cref{fig:api} can solve this problem. (By this reasoning we also deduce that a pure implementation of that interface indeed does not exist.)

So does this invalidate our claims that we built a beautiful, well-behaved, pure abstraction? In the end that depends on your expectation: Do you expect that in a pure and lazy language lambda-lifting preserves meaning, including termination? Or is that an additional property of some languages, which we can maybe do without?

As you ponder this question, note that if you include (asymptotic) resource usage in your expectation of program equivalence, then such lambda-lifting is already rather dangerous in Haskell: The nice and fast knot-tied |fibs| from the introduction becomes horribly inefficient once you turn it into a recursive \emph{function} (|let fibs x y = x : y  : zipWith (+) (fibs x y) (tail (fibs x y))|). Because the lambda-lifting transformation is only an equivalence as long as we ignore (asymptotic) complexity, it seems that breaking that equivalence is not too wicked.

\Citet[Fact 3.7]{sabry} points out that merely changing the set of observational equivalences does \emph{not}, in general and on its own, imply that the extended language is no longer pure.

\subsection{Compiler transformations}\label{sec:comptrans}

The compiler tends to share the attitude that (at least asymptotic) complexity is a relevant aspect of the program's semantics, and will thus be careful to preserve (recursive) sharing that is explicit in the source program. Therefore we can be fairly certain that it will not just willy-nilly apply such lambda-lifting to our code.

The same holds for all the other transformations applied by the compiler, such as partial evaluation, case-of-case, worker/wrapper, common subexpression elimination etc.\ \citep{optimiser}. Because our library has these nice equational properties as long as sharing is preserved, we can be fairly certain that all these are still meaning-preserving in the presence of our library.

\subsection{Sabry's criterion}

One possible definition for purity is offered by \citet{sabry}:
\begin{quote}
A language is purely functional if (i) it includes every simply typed λ-calculus term, and
(ii) its call-by-name, call-by-need, and call-by-value implementations are equivalent (modulo
divergence and errors).
\end{quote}

It is not straight-forward to apply this criterion to our library, or Haskell itself, because it assumes the existence of these three implementations, and we do not have that for Haskell. Even if we had, in the call-by-value implementation, all our recursive definitions would vacuously loop, and in the call-by-name implementation (quite like after the lambda lifting), our library would not see \emph{finite} graphs, so all interesting programs would become non-terminating. In that sense, our library \emph{is} pure according to Sabry’s criterion.

Because it requires multiple implementations we find Sabry’s criterion of purity not fully convincing. Maybe there exists a suitable criterion that works with a single language semantics, maybe by listing observable equivalences that must hold for a language to be considered pure?
We'll leave this question hanging, hoping it can tempt someone to give a satisfying answer.

\subsection{Longley’s criterion}

\Citet{longley} makes the observation that there exist functions with perfectly pure and functional \emph{behavior} that cannot be \emph{implemented} in the pure fragment of the language, but can be implemented in a suitable extension (ML with references in his case). Our library is an example of that: It behaves pure and functional, but needs extensions (like |unsafePerformIO|) to be implemented.

Longley considers a term to be functional if it denotes an element in the mathematical denotation of its type. At function type, this means they are extensional and that they are described by a mathematical function. We struggle applying this criterion directly, since we have not yet managed to find a denotational semantics for Haskell extended with our library. Unlike usual call-by-name semantics for pure functional programming languages, it must be be able to tell knot-tying recursion apart from unrestricted recursion. It remains to be seen if we can find a suitable denotational semantics.

\fi

\end{document}
