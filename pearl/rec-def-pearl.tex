\documentclass[manuscript,anonymous,screen,acmsmall]{acmart}

\let\Bbbk\undefined %https://github.com/kosmikus/lhs2tex/issues/82
\typeout{(./polytt.fmt)} % For latexmk
%include polytt.fmt
%format IND = "~~"
%format -> = "–>"
%format <- = "<–"
%format :: = "::"
%format (QUOTETYPE(x)) = "\textquotesingle\textquotesingle " x
%format (QUOTEVAL(x)) = "\textquotesingle " x
%format (PRAGMA(x)) = "\{−\# " x " \#−\}"
%format (FILECHECK(x)) = "−− " x
%format (LHPRAGMA(x)) = "\{−@ " x " @−\}"
%format ghci = "\textbf{ghci}"

\renewcommand{\onelinecomment}{\quad–\hspace{.5pt}–\itshape{}}
\renewcommand{\hsindent}[1]{{\quad\strut}}
%subst newline          = "\\[-0.3ex]%'n"
\setlength{\blanklineskip}{0.8ex}



% Ubuntu Light for code
\usepackage{fontspec}
\setmonofont{Ubuntu-R}%
  [ Scale = MatchLowercase
  , BoldFont = Ubuntu-B
  , BoldItalicFont = Ubuntu-BI
  , ItalicFont = Ubuntu-RI
  , WordSpace = {1,2,1}
  ]

% URL fix
\makeatletter
% Inspired by http://anti.teamidiot.de/nei/2009/09/latex_url_slash_spacingkerning/
% but slightly less kern and shorter underscore
\let\UrlSpecialsOld\UrlSpecials
\def\UrlSpecials{\UrlSpecialsOld\do\/{\Url@@slash}\do\_{\Url@@underscore}}%
\def\Url@@slash{\@@ifnextchar/{\kern-.07em\mathchar47\kern-.2em}%
   {\kern-.0em\mathchar47\kern-.08em\penalty\UrlBigBreakPenalty}}
\def\Url@@underscore{\nfss@@text{\leavevmode \kern.06em\vbox{\hrule\@@width.3em}}}
\makeatother


\usepackage[capitalise]{cleveref}
\usepackage{textgreek}

\setcopyright{rightsretained}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{XXXXXXX.XXXXXXX}


\acmConference[ICFP'23]{ICFP}{June 03--05,2018}{Woodstock, NY}

\citestyle{acmauthoryear}

\begin{document}

\title{Functional Pearl: More fixpoints!}

\author{Joachim Breitner}
\email{mail@@joachim-breitner.de}
\orcid{0000-0003-3753-6821}
\affiliation{%
   \institution{unaffiliated}
%  \institution{Institute for Clarity in Documentation}
%  \streetaddress{P.O. Box 1212}
%  \city{Dublin}
%  \state{Ohio}
  \country{Germany}
%  \postcode{43017-6221}
}


\begin{abstract}
Haskell’s laziness allows the programmer to solve some problems naturally and declaratively via recursive equations. Unfortunately, if the input is “too recursive”, these very elegant idioms can fall into the dreaded black hole, and the programmer has to resort to more pedestrian approaches.

It does not have to be that way: We built variants of common pure data structures (Booleans, sets) where recursive definitions are productive. Internally, the infamous \texttt{unsafePerformIO} is at work, but the user only sees a beautiful and pure API, and their pretty recursive idioms -- magically -- work again.

%In the end, this raises interesting questions about the precise nature of purity.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011008.10011024.10011033</concept_id>
       <concept_desc>Software and its engineering~Recursion</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011008.10011009.10011012</concept_id>
       <concept_desc>Software and its engineering~Functional languages</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Recursion}
\ccsdesc[500]{Software and its engineering~Functional languages}

\keywords{Haskell, recursion, fixpoint}

\maketitle

\section{Introduction}

Haskell is a pure and lazy programming language, and this laziness allows us to express some algorithms very elegantly, by recursively referring to currently calculated values. A typical and famous example is the following definition of the Fibonacci numbers as an infinite stream:
\begin{code}
fibs :: [Integer]
fibs = 0 : 1 : zipWith (+) fibs (tail fibs)
\end{code}
This is often called “knot-tying”, because a value (here \verb|fibs|) has a definition involving itself.

\subsection{Tying the knot with graphs}

A maybe more practical example is the following calculation of the reflexive transitive closure of a graph, i.e.\ for each node the set of nodes reachable from it. Let's represent a graph as a map from vertices (\verb|Int|) to lists of adjacent vertices:
\begin{code}
import qualified Data.Set as S
import qualified Data.Map as M
type Graph = M.Map Int [Int]
\end{code}

Then the reflexive transitive closure can be very elegantly expressed by knot-tying a map from vertices to the set of reachable vertices:
\begin{code}
transitive1 :: Graph -> Graph
transitive1 g = M.map S.toList sets
  where
    sets :: M.Map Int (S.Set Int)
    sets = M.mapWithKey (\v vs -> S.insert v (S.unions [ sets M.! v' | v' <- vs ])) g
\end{code}
This code is quite close to the prosaic specification “the reachable nodes from a node \verb|v| are \verb|v| itself, plus all the nodes reachable from its successors”; hence we can call this code declarative.

Note how the definition of \verb|sets| refers to itself -- “we are tying a knot”.

\subsection{It works, until it doesn't}

This is the kind of code we like to impress our strict-language-using friend with, and it works quite nicely:
\begin{code}
ghci> transitive1 (M.fromList [(1,[3]),(2,[1,3]),(3,[])])
fromList [(1,[1,3]),(2,[1,2,3]),(3,[3])]
\end{code}
%
At least until our strict-language-using friend challenges us to add just one small edge to the graph:
\begin{code}
ghci> transitive1 (M.fromList [(1,[2,3]),(2,[1,3]),(3,[])])
fromList [(1,fromList ^CInterrupted.
\end{code}
Why does it fail? Because the graph has a \emph{cycle} ($1 \to 2 \to 1$), and that makes our code get lost in an infinite loop, until we abort the program.

This is quite disappointing, because in order to handle recursive graphs as input, we now have to implement this in a much more tedious way, maybe with an explicit loop, keeping track of the set of seen vertices (see \cref{sec:imp} if you really want to see it, but the point is that you shoudn’t have to). It works, and most of us have written that idiom once (or many times), but we cannot impress our friend with that.

But it seems we should: The declarative specification, from which we have derived \verb|transitive1|, holds for recursive graphs as well, so it does not seem that unreasonable to expect our code to handle recursive graphs as well. Where does it go wrong? The way we use the lazy map data structure is fine; it helps us to express the set of reachable nodes by way of other such sets.
The problem is that the set data structure, with its operations \verb|insert| and \verb|union|, is not lazy enough: \verb|union| needs to know the value of its arguments before it can produce something useful, and thus cannot be used recursively.

\subsection{We need better sets!}

In this paper we present a data structure for sets, called \verb|RSet|, where such recursive expressions do work! Its API is almost the same as that of \verb|Data.Set|. In particular, it consists of plain \emph{pure} functions -- no monads necessary. The fragment relevant for our example is:
\begin{code}
insert  :: Ord a =>  a -> RSet a  -> RSet a
unions  :: Ord a =>  [RSet a]     -> RSet a
get     ::           RSet a       -> Set a
\end{code}
In addition to the two operations used by \verb|transitive1|, with type signatures mirroring those of \verb|Data.Set| exactly, we find the function \verb|get| to convert our \verb|RSet| to a normal \verb|Set|.

We can use this data structure without changing the structure of our code; we just swap out the operations (imported qualifiedly as \verb|RS|) and convert back to conventional sets in the end:
\begin{code}
transitive2 :: Graph -> Graph
transitive2 g = M.map (S.toList . RS.get) sets
  where
    sets :: M.Map Int (RS.RSet Int)
    sets = M.mapWithKey (\v vs -> RS.insert v (RS.unions [ sets M.! v' | v' <- vs ])) g
\end{code}

And indeed, now we \emph{can} handle recursive graphs, and get the correct result:
\begin{code}
ghci> transitive2 (M.fromList [(1,[2,3]),(2,[1,3]),(3,[])])
fromList [(1,[1,2,3]),(2,[1,2,3]),(3,[3])]
\end{code}

\subsection{Contributions}

From the user’s point of view, that’s almost all there is  to say: There is a library of types (sets, Booleans, maps) you can use like the conventional types, and suddenly your favorite knot-tying tricks work even better. In \cref{sec:exploration} we'll explore this library from the user's point of view, followed by a larger program analysis case-study (\cref{sec:casestudy}) , before taking a look at how it works under the hood (\cref{sec:impl}), discussing whether calling this \emph{pure} is actually warranted (\cref{sec:pure}) and finally taking a brief glance at related approaches (\cref{sec:related}).

\medskip

The main contributions of this paper are:

\begin{itemize}
\item We show that by making more data types recursively definable, more problems can be solved elegantly and declaratively. They are \emph{safe} and \emph{pure} and are a natural fit for a language like Haskell.

\item We describe the implementation with such variants of common data types (sets and Booleans, and extensible to more types), as a regular library using GHC's \verb|unsafePerformIO| primitive under the hood.\footnote{The library can be found as \texttt{rec-def} on Hackage; an anonymized copy is included in this submission.}

\item By wondering what it even means for a language to be considered \emph{pure} (let alone how to formally prove it), posing a challenge to the community.

%\item observing, as a hopefully enlightening insight, that one of the main features of \emph{laziness} is that it allows more recursive equations to be solved, and thus more problems be expressed declaratively, and thus a the lazier the language, the more declarative.
\end{itemize}

\begin{acks}
We like to thanks Sebastian Graf for their helpful comments.
\end{acks}

\section{Exploration}\label{sec:exploration}

In the introduction we have used a data type \verb|RSet| with an API that resembles that of the \verb|Set| data structure in Haskell's \verb|Data.Set| library. Let us explore this data structure some more from the user's point of view, to get a better understanding of how it is different from the vanilla \verb|Set|, and to whet the appetite for the look at its implementation in a subsequent section.
\Cref{fig:api} gives a comprehensive overview of the API.

\subsection{Just a isomorphic copy?}

\begin{figure}
\begin{code}
module Data.Recursive.Set where  -- imported as RS here
  data RSet a
  get           ::           RSet a ->            Set a
  mk            ::           Set a ->             RSet a
  empty         ::                                RSet a
  singleton     ::           a ->                 RSet a
  insert        :: Ord a =>  a -> RSet a ->       RSet a
  delete        :: Ord a =>  a -> RSet a ->       RSet a
  union         :: Ord a =>  RSet a -> RSet a ->  RSet a
  unions        :: Ord a =>  [RSet a] ->          RSet a
  intersection  :: Ord a =>  RSet a -> RSet a ->  RSet a
  member        :: Ord a =>  a -> RSet a ->       RBool
  null          ::           RSet a ->            RDualBool
  when          ::           RBool -> RSet a ->   RSet a
  id            ::           RSet a ->            RSet a

module Data.Recursive.Bool where  -- imported as RB here
  data RBool
  get          :: RBool ->           Bool
  mk           :: Bool ->            RBool
  true, false  ::                    RBool
  (&&),(||)    :: RBool -> RBool ->  RBool
  and, or      :: [RBool] ->         RBool
  not          :: RBool ->           RDualBool
  id           :: RBool ->           RBool

module Data.Recursive.DualBool where  -- imported as RDB here
  data RDualBool
  get          ::  RDualBool ->               Bool
  mk           ::  Bool ->                    RDualBool
  true, false  ::                             RDualBool
  (&&),(||)    ::  RDualBool -> RDualBool ->  RDualBool
  and, or      ::  [RDualBool] ->             RDualBool
  not          ::  RDualBool ->               RBool
  id           ::  RDualBool ->               RDualBool
\end{code}
\caption{The API of recursively definable sets and Booleans}\label{fig:api}
\end{figure}

At the first glance, \verb|RSet| looks like a isomorphic copy of \verb|Set|, with \verb|get :: RSet a -> Set a| and \verb|mk :: Set a -> RSet a| converting between the types, and all the operations on \verb|RSet| behave as their counterpart on \verb|Set|. Let's quickly check that \citep{quickcheck}:
\begin{code}
ghci> quickCheck $ \s -> RS.get (RS.mk s) === s
+++ OK, passed 100 tests.
ghci> quickCheck $ \s1 s2 -> RS.get (RS.union s1 s2) === S.union (RS.get s1) (RS.get s2)
+++ OK, passed 100 tests.
\end{code}

The second equation generalizes to all operations in the API, giving them a straight-forward specification in terms of the corresponding operation on the underlying vanilla data type. But there must be a difference, else we would not be writing this paper.

\subsection{Recursion!}

The difference is that with \verb|RSet|, \emph{recursively defined expressions work!} For example, using vanilla \verb|Set| from \verb|Data.Set| (imported qualified as \verb|S|), evaluating recursive expressions tends to hang:
\begin{code}
ghci> let s = S.insert 42 s in s
fromList ^CInterrupted.
\end{code}
With \verb|RSet|, it simply works:
\begin{code}
ghci> let s = RS.insert 42 s in RS.get s
fromList [42]
\end{code}
It works for larger expressions as well
\begin{code}
ghci> let s = RS.insert 42 (RS.union (RS.insert 23 s) (RS.delete 42 s)) in RS.get s
fromList [23,42]
\end{code}
Not even mutual recursion poses a problem:
\begin{code}
ghci> let  s1 = RS.insert 42 s2
ghci|      s2 = RS.insert 23 s3
ghci|      s3 = RS.delete 42 s1
ghci| in (RS.get s1, RS.get s2, RS.get s3)
(fromList [23,42],fromList [23],fromList [23])
\end{code}

In these examples, we build the graph of recursively defined \verb|RSet| values explicitly, using \verb|let|. In practice one would more likely construct that graph using lazy data structures and knot-tying, maybe dynamically based on some input, as done in the introduction.

\subsection{Fixpoints}

It may come as a pleasant surprise that these expressions are productive, i.e.\ that we even obtain a result. But is it the right result? If we look at the last example above we can see that the vanilla sets we get for each of the three variables makes the three defining equations true:
\begin{code}
ghci> let s1 = S.fromList [23,42]; s2 = S.fromList [23]; s3 = S.fromList [23]
ghci> s1 == S.insert 42 s2
True
ghci> s2 == S.insert 23 s3
True
ghci> s3 == S.delete 42 s1
True
\end{code}

That is good, because that is how we want equations in a functional programming language to behave.

At this point you might interject that these are not the only possible solutions to this system of equations. Returning to the smaller example of \verb|let s = RS.insert 42 s|, we find that our result \verb|S.fromList [42]| indeed solves the equation \verb|s == S.insert 42 s|, but so does \verb|S.fromList [42,43]|. Still, we would not consider that a “good” solution, and would be surprised if we'd get that.
%
That is because we expect the result to be \textbf{least fixpoint}; the solution that is, among all possible solutions, the smallest with regard to a particular partial order.

In the context of sets the natural order is subset inclusion. Therefore, a possibly recursive expression of \verb|RSet| values will evaluate to the smallest sets solving the definitional equations.

It will always do so, provided that
\begin{itemize}
\item only finitely many \verb|RSet| values are involved and
\item that the definition of a \verb|x| does not depend on \verb|RS.get x|
\end{itemize}
Using \verb|RS.get| drops us in the world of vanilla set, and the magic disappears:
\begin{code}
ghci> let s = RS.mk (RS.get (RS.insert 42 s)) in RS.get s
fromList ^CInterrupted.
\end{code}
In this sense, \verb|RS.mk . RS.get| is not the identity function.

\subsection{More than sets}

Our library of recursively definable values does not only provide sets, but also other data types, in particular Booleans, as seen in \cref{fig:api}. Again, we have a copy of the usual operations (literals, conjunction and disjunction), and as before, we expect a (possibly recursive) expression of \verb|RBool| values to evaluate to the Boolean that solves these equation.

What happens if both \verb|True| and \verb|False| would solve an equation, like in the following case?
\begin{code}
ghci> let x = x RB.|| x in RB.get x
False
\end{code}
We can see that \verb|RBool| considers \verb|False| as the least element, and for some use-cases that is the right choice. But for other use-cases, one would prefer \verb|True| over \verb|False|. Therefore, the library proves a separate module and data type \verb|RDualBool|, again with the full set of operations on Booleans, but this time returning \verb|True| if possible:
\begin{code}
ghci> let x = x RDB.|| x in RDB.get x
True
\end{code}

\subsection{Monotonicity}\label{sec:monotonicity}

These data types -- \verb|RSet|, \verb|RBool|, \verb|RDualBool| are not silos, and you will find among the functions in \cref{fig:api} some that connect these types -- negation on Booleans, member checks on sets, the emptiness check on sets, and the function \verb|when|, which guards the values of a set on a boolean.
This means that even recursive expressions involving multiple of these types will produce a result.

So why does \verb|RS.member| return a \verb|RBool|, but \verb|null| returns a \verb|RDualBool|? And why is there no function \verb|not :: RBool -> RBool|? It is because all functions involved here must be \emph{monotonic}: smaller arguments must lead to smaller results. And because the empty set is smaller than non-empty sets, \verb|RS.member| must return a \verb|RBool| (where \verb|False| is smaller than \verb|True|), but \verb|null| must go to \verb|RDualBool| (where \verb|True| is smaller than \verb|False|).

If we did not pay attention to this while defining the API, and added non-monotonic functions (like \verb|not :: RBool -> RBool|), the user would be  able to write equations that do not have a solution, such as
\verb|let x = not x|,
and we would like to statically rule that out.

The underlying bit of theory is the theorem that a monotone function $f : A \to A$ on a partially ordered set $A$ with least element $\bot \in A$ and finite height has a unique least fixed point. This is well-known (e.g.\ \citet{lazyleast}), and if the sentence means something to you, you probably already saw it coming. And if it doesn't, it does not matter for reading this paper.

This also explains why some functions from the underlying vanilla data type (such as \verb|Data.Set|'s \verb|difference|) are not available, as they are not monotonic.

\subsection{Termination}

Another function from \verb|Data.Set| that we do not have in \verb|Data.Recursive.Set| is the function \verb|map :: Ord b => (a -> b) -> Set a -> Set b|. This may be a bit surprising, as this function is perfectly monotonic with regard to the subset relation. But it can cause other problems: Imagine we had it, and wrote
\begin{code}
let s = RS.insert 0 (RS.map (+1) s) in RS.get s
\end{code}
Does this equation have a solution? Clearly the set \verb|s| needs to contain \verb|0|. But then it also needs to contain \verb|1|. And \verb|2|. And so on. So the solution would have to be the set of \emph{all} natural numbers, but that is not something that \verb|Data.Set|, being a data structure of \emph{finite} sets, can represent.

So we cannot allow this function for \verb|RSet| if we want to guarantee a result for every finite recursive expression.

For the theoretically inclined, this plays into the “$A$ has finite height” requirement in the theorem above. You might be irked that the type \verb|Set a|, ordered by subset inclusion, does not actually have finite height (if \verb|a| is not finite). With the current API (without functions like \verb|map|) for every \emph{finite} \verb|RSet| expression there are only finite many possible members, and thus the relevant “subtype” has finite height, and all is well again.

It would not be unreasonable, however, to add \verb|map| to the \verb|RSet| API, as it may be quite useful for some applications, and maybe in these applications the equation have a finite solution just fine. If we'd do that, we could no longer \emph{guarantee} termination for all possible expressions (as shown by the example above), but if expression yields a result, it will be the least fixed point of the defining equation. One can argue that this would be fine for a Haskell library, as Haskell programmers are used to programming around non-termnation anyways.

\subsection{The black hole}\label{sec:blackhole}

We said that “all finite, possibly recursive expressions yield a result”. Unfortunately, that is not completely true: If a value of type \verb|RSet| is defined to be simply itself, with none of the \verb|RSet| operations involved, it will not work:
\begin{code}
ghci> let s = s in RS.get s
fromList ^CInterrupted.
\end{code}
And it’s not for lack of a solution: Clearly the empty set is the least solution to the equation \verb|s == s|.

Because our library is but a library, despite the apparent magic inside (which we will uncover in \cref{sec:impl}), with a definition like \verb|let s = s|, it has no chance to insert its magic.

The problem goes away as soon as any function from the API is involved in the definition, even if it is semantically the identity:
\begin{code}
ghci> let x = RS.unions [x] in RS.get x
fromList []
\end{code}

This is a little stumbling block when using this library. And while programmers are unlikely to write \verb|let x = x| directly, the effect can occur when tying the knot via a lazy data structure. In that case, the programmer is advised to insert a semantic identity function in the right position; the function \verb|RS.id :: Set a -> Set a| can be used for that purpose.
A programming language that integrates these features first-class can take care of this automatically.

\section{Case study: a program analysis}\label{sec:casestudy}

Before we leave the user's point of view and look under the hood of the library, let us conclude the section with a slight larger and more realistic use-case. We hope that this example shows that using recursively definable values can make the code noticeably more declarative and elegant.

\subsection{First without recursion}

The example is a small program analysis of a functional language with lazy let-bindings, (mutual) recursion and exceptions that determines whether evaluating an expression may throw an (uncaught) expression.

To set the stage, \cref{fig:analast} contains a Haskell Datatype for a typical abstract syntax. The \verb|LetRec| constructor takes a list of bound variables along with their definition's  right-hand side, and a body; all bound variables are in scope in all the right-hand sides and the body.
Variable names bound in \verb|Lam|, \verb|Let| and \verb|LetRec| shadow outer occurrences. (The resemblance to GHC's intermediate language Core \citep{secrets} is certainly not a coincidence.)

\begin{figure}
\begin{code}
type Var  =  String
data Exp  =  Var Var | Lam Var Exp | App Exp Exp | Throw | Catch Exp
          |  Let Var Exp Exp | LetRec [(Var, Exp)] Exp
\end{code}
\caption{An AST for a functional language with mutual recursion and exceptions}
\label{fig:analast}
\end{figure}

Let us ignore \verb|LetRec| at first, and write that analysis as a simple traversal of the AST:
\begin{code}
canThrow :: Exp -> Bool
canThrow e = go M.empty e
  where
    go :: M.Map Var Bool -> Exp -> Bool
    go env (Var v)        = env M.! v
    go env Throw          = True
    go env (Catch e)      = False
    go env (Lam v e)      = go (M.insert v False env) e
    go env (App e1 e2)    = go env e1 || go env e2
    go env (Let v e1 e2)  = go env' e2
      where
        env_bind  = M.singleton v (go env e1)
        env'      = M.union env_bind env
\end{code}

To determine whether evaluating a varible can throw, because of laziness, we have to carry around and
environment of type \verb|M.Map Var Bool| where we remember whether the corresponding right-hand could throw. \verb|Throw| and \verb|Catch| certainly can resp.\ cannot throw. The analysis isn't higher order, so for lambdas we assume they can throw their body can throw, and for applications when either subexpression can throw. And finally for \verb|Let|, we extend the environment with the analysis result of the bound variable and descend.

\subsection{Avoiding the black hole}
So far, so standard. But what about \verb|LetRec|? Here, the analysis result of each right-hand side depends on the analysis results of all right-hand sides. We can try to simply do what we did in the \verb|Let| case:
\begin{code}
    go env (LetRec binds e) = go env' e
      where
        env_bind  = M.fromList [ (v, go env' e) | (v,e) <- binds ]
        env'      = M.union env_bind env
\end{code}
Note that, crucially, we use the extended environment \verb|env'| not only for the body, but also for the right-hand sides.

Alas, this does not work: As soon as we try to analyze an expression that uses recursion, we will fall into a black hole. The crux is that \verb|Bool| is not recursively definable, because the operations (here only disjunction, \verb+(||)+) is not lazy.

But if we use \verb|RBool| instead of \verb|Bool|, it just works:
\begin{code}
canThrow :: Exp -> Bool
canThrow e = RB.get (go M.empty e)
  where
    go :: M.Map Var RBool -> Exp -> RBool
    go env (Var v)          = env M.! v
    go env Throw            = RB.true
    go env (Catch e)        = RB.false
    go env (Lam v e)        = go (M.insert v RB.false env) e
    go env (App e1 e2)      = go env e1 RB.|| go env e2
    go env (Let v e1 e2)    = go env' e2
      where
        env_bind  = M.singleton v (go env e1)
        env'      = M.union env_bind env
    go env (LetRec binds e) = go env' e
      where
        env_bind  = M.fromList [ (v, RB.id (go env' e)) | (v,e) <- binds ]
        env'      = M.union env_bind env
\end{code}
All we did was to change the type of the local function \verb|go| to use \verb|RBool| instead of \verb|Bool|, used the corresponding operations (\verb|RB.true|, \verb|RB.false| and \verb+RB.||+) and  projected out to normal Booleans at the end (using \verb|RB.get|). A slight blemish is that we also inserted a call to \verb|RB.id| so that we don't fall over the input \verb|LetRec [("x", Var "x")]|, as explained in \cref{sec:blackhole}.

\subsection{Results everywhere}

To keep it simple for this paper the analysis does not update the AST (e.g. remove redundant calls to \verb|Catch|, or update analysis information stored in the variables as GHC would do), as the necessary plumbing would obscure our point. But it would work: we can use \verb|RB.get|  with in the function to get the analysis result for any subexpression and return a changed AST accordingly, e.g.
\begin{code}
    go :: M.Map Var RBool -> Exp -> (RBool, Exp)
    ...
    go env (Catch e)        = (RB.false, new_e)
      where
        (can_throw, e') = go env e
        new_e  |  RB.get can_throw  = Catch e'
               |  otherwise         = e'
\end{code}
We'd have to be careful that the \verb|RBool| returned by \verb|go| does not depend on any decision made based on a boolean that was obtained with \verb|RB.get|. For such a fused analysis/transformation pass this is typically possible.
%The ability to query each “node” as we go sets this approach apart from other idioms with a

\subsection{Alternatives}
What would we do if we did not have \verb|RBool| at our disposal? Here are some common options
\begin{itemize}
\item We can perform an explicit fixpoint analysis in the \verb|LetRec| case: Initialize the \verb|env_bind| with all variables mapped to \verb|False|, descend, check if now any analysis result has changed, and if so, re-analyze all of them, until we found a solution.

Maybe we can be more clever and only re-analyze some of them.

Maybe some of that logic can be extracted into a suitable fixpoint operator.

In any case, we would obscure the declarative intent of the code with lower-level bookkeeping.

\item If we do that naively, we can run into the problem that in the presence of nested recursive lets, the nested fixpoint iteration can cause exponential complexity.

In the case where the analysis result is persisted as annotations in the syntax tree anyways, we can address this issue by starting the fixpoint iteration not from bottom, but from the result of the previous outer iteration.

GHC does this, as explained in \citet[Section 6.6]{modular}, but again at the cost of more plumbing obscuring the code's intent.

\item Another way to approach the problem is to gather the full data flow problem from the whole AST, solve it globally (thus also avoiding the problem mentioned in the previous bullet), and then distributing the analysis again to where we need it. This is reminiscent of how a constrained-based type inference algorithm works.

It is a satisfying thought that this approach frees the solving algorithm from having to follow the syntactic nesting structure of the program, and that the repeated passes of the fixpoint iteratation do not have to re-analyze the AST over and over, and instead only sees the pure data-flow equations.

There are, however, petty practical issues with this approach (representing the equations as data, uniquely nameing the cells, the separate passes for collecting the equations and using the results).
At this point, one is likely going to hide this bookkeeping in a suitable monad, which can recover some of the lost elegance, but if the code could otherwise be written as pure functions, that is still quite a price to pay.%
\footnote{See \url{https://hackage.haskell.org/package/datafix-0.0.1.0/docs/Datafix-Denotational.html\#v:datafix} for an attempt.}
\end{itemize}
When our library is applicable, it allows us to retain the concise elegance of the pure code that does not bother with the “how” of solving equations, while under the hood the solver has a comprehensive global view of the problem.


\section{Under the hood}\label{sec:impl}

We hope that by now you are eager to learn how the \verb|RSet| library is implemented. It is a regular Haskell library, without dedicated compiler support nor using compiler plugins. Maybe this sounds impossible, and we agree: The API and specification presented in the previous section \emph{cannot} be implemented in normal, safe, pure Haskell code.\footnote{At least we believe it is not possible, as we explain in \cref{sec:sat}.}

But it can be implemented using \emph{“unsafe”} features; in particular GHC's infamous function \verb|unsafePerformIO :: IO a -> a|, which allows arbitrary side-effects in pure code. Before you turn away in disgust please allow us to quote \citet{unsafePerformIO} from their publication introducing this primitive:
\begin{quote}
However “unsafe” is not the same as “wrong”. It simply means that the programmer, not the compiler, must undertake the proof obligation that the program's semantics is unaffected by the moment at which all these side effects take place. [\ldots]
So, we regard the primitives of this paper as \emph{the raw material from which experienced systems programmers can construct beautiful abstractions}.
\end{quote}
This is our goal; whether the abstraction presented in \cref{sec:exploration} is beautiful is in the eye of the beholder.

\subsection{A naive implementation}

The core idea behind the implementation can be explained in two simple steps: First, we use an imperative API to declare values, register their relationships and read their values, and then we wrap that in a pure and sufficiently lazy wrapping. We begin by outlining a naive implementation that initially ignores issues of reentrancy-safety, modularity, performance and space-leaks.

\subsection{An imperative core}\label{sec:impcore}

A typical \emph{imperative} API to describe and let solve a set of recursive equations typically provides functions to (1) register the variables, or \emph{cells}, (2) define their relationships and (3) finally read their values. To keep the example code small, we focus on just sets and insertion as the only operation, and could imagine an API like the following:
\begin{code}
data Cell a
newCell       ::  IO (Cell a)
cellIsInsert  ::  Ord a => Cell a -> a -> Cell a -> IO ()
getCell       ::  Cell a -> IO (Set a)
\end{code}

A typical use of this API, solving a mutually recursive set equation, could be
\begin{code}
ghci> c1 <- newCell
ghci> c2 <- newCell
ghci> cellIsInsert c1 42 c2
ghci> cellIsInsert c2 23 c1
ghci> getCell c2
fromList [23,42]
\end{code}

At this point, the actual implementation of this API is not that interesting. In a simple implementation a cell would consist of a current value (initialized to the empty set), and a list of cells depending on this value, and then the changes due to calls to \verb|cellIsInsert| are propagated through this network until no more changes need to be propagated. Of course, more sophisticated algorithms may lurk underneath this interface.

\subsection{The pure wrapping}

The more interesting question is how to get from the imperative \verb|Cell| code to the pure \verb|RSet| API, with \verb|insert :: a -> RSet a -> RSet a| and \verb|get :: RSet a -> Set a|?

Clearly, \verb|insert| must somehow both create a new cell, and define its equation.
Furthermore, it has to be lazy in its second argument, else a recursive equation would immediately loop, so it somehow has to defer the call to \verb|cellIsInsert| a bit. This leads to the code seen in \cref{fig:wrap}, which we go through in detail:

\begin{figure}
\raggedright
\begin{code}
data RSet a = RSet (Cell a) (IO ()) (IORef Bool)

insert :: Ord a => a -> RSet a -> RSet a
insert x r2 = unsafePerformIO $ do
   c1 <- newCell
   done <- newIORef False
   let todo = do
            is_done <- readIORef done
            unless is_done $ do
                writeIORef done True
                let (RSet c2 todo2 _) = r2
                cellIsInsert c1 x c2
                todo2
   return (RSet c1 todo done)

get :: RSet a -> Set a
get (RSet c todo _) = unsafePerformIO $ do
   todo
   getCell c
\end{code}
\caption{Wrapping an imperative propagator library in a pure way}\label{fig:wrap}
\end{figure}

A value of type \verb|RSet| consists of three fields
\begin{itemize}
\item The \verb|Cell a| backing the value we are defining.
\item An \verb|IO ()| action, deferred until the the value is actually needed (directly due to \verb|get|, or indirectly).
\item A flag to remember if that deferred action has run already.
\end{itemize}

The function \verb|get| does not do much: It triggers the \verb|todo| action, and afterwards returns the current value of the cell. The interesting bits are in the \verb|insert| function: It creates a new cell to represent the result, and a “done”-flag. It returns these together with a \verb|todo|-action, which is \emph{not yet run}. Note that the second argument, \verb|r2|, is \emph{not} looked at yet, so \verb|insert| is lazy, as required.

The \verb|todo| action itself uses the flag to ensure it is only run once. Only now the value \verb|r2| is analyzed, and the relationship between the cells is registered. It also runs the \verb|todo| action of the other cell. This way, a single call to \verb|get| will recursively trigger the \verb|todo| actions of all involved values -- and the flags prevent that process from running in cycles.

\subsection{Less naively, please}

This code describes the essence of our idea, and easily generalizes to the other operations of the \verb|RSet| API. It is, however, naive in a few ways that are worth discussing.

\subsubsection{Reentrancy and thread safety.}\label{sec:thread}

The \verb|done| flag is used to ensure that the \verb|todo| action is run exactly once. But if \verb|get| is invoked concurrently, the code above is obviously racy. Even worse: Because this is run from \verb|unsafePerformIO|, even in a single-threaded environment we have to worry about reentrancy. In our library we address this with careful use of \verb|MVar| \cite{concurrent}, and hide this cleanly behind a small abstraction for “possibly recursive \verb|IO|-thunks” in the \verb|System.IO.RecThunk| API.

The dejafu test library \citep{dejafu}, which can exhaustively explore all possible interleavings of concurrent code, has proven invaluable: more than once we thought we had finally achieved thread-safety, only to be told that we were still far off.

\subsubsection{Modularity.}

The naive code above supports just one value type, \verb|Set|, because the underlying imperative propagator library \verb|Cell| only supports that type. The full library abstracts over the underlying propagator. This way we can have recursively defined values of different types (\verb|RSet a|, \verb|RBool|), operations connecting them (e.g.\ \verb|member|) and allow solving heterogenously typed sets of equations.

Supporting different propagator libraries also opens the way for important performance optimizations that are specific to various value types. The most generic propagator implementation assumes no structure on its values besides equality, and just keeps propagating changes until the graph has stabilized. But if we can have different propagator implementations for different types, smarter propagator libraries can be written.

For example for Booleans, a cell changes its value at most once, from \verb|False| to \verb|True|. Once it is already \verb|True|, it will never change again, and one can drop its connection to other cells (see the \verb|Data.Propagator.P2| module).

Similarly, a propagator library for finite sets can propagate just deltas, instead of always recomputing the sets from its full inputs, to avoid repeating work. (This is not yet implemented in our library, but would be possible without affecting the public API.)

\subsubsection{Space leaks}\label{sec:spaceleak}

Another, maybe subtle, problem with our pure wrapping of an imperative propagator library is that it can easily lead to space leaks.

Consider the \verb|insert| function. With \verb|rs2 = RS.insert x rs1| we create a new mutable cell for \verb|rs2|, and tell the mutable cell in \verb|rs1| to notify the other cell of any changes. This means that now somewhere in \verb|rs1| there is a reference to \verb|rs2| -- exactly the other way around from what one would expect from the code. This prevents resources allocated for \verb|rs2| from beeing freed while \verb|rs1| is alive.

This causes space leaks, where allocated resources are kept alive longer than expected or needed. As an extreme example, consider now this interaction
\begin{code}
ghci> RS.get (RS.insert 42 RS.empty)
fromList [42]
\end{code}
The call to \verb|insert| allocates a new cell and registers it with the cell in \verb|RS.empty|. But \verb|RS.empty| is a \emph{static} value, and will never be garbage collected!

To fix this, we notice that once we are done calculating a value (either because we query it using \verb|get|, or because it is needed in the calculation of such a value), it will never change any further. So our propagator API allows “freezing“ a cell and dropping the connection to the depenent cells, and our pure wrapper freezes cells after the value is computed.

\section{Is this still Haskell?}\label{sec:pure}

The library presents itself with a innocent looking, pure and simple interface (\cref{fig:api}), but the implementation is full of side-effects (as seen in \cref{sec:impl}).
Does this leak, or is does programming with it still feel like programming Haskell?
Is this really pure? Did we create a beautiful abstraction, or are the types a lie?

\subsection{What does purity even mean?}

We found that this question is not easy to answer, because there does not exist a single, clear and widely accepted definition of “pure”.
This paper is not the place to give an authoritative answer, so we look at various language properties commonly associated with a pure functional language in general and Haskell in particular,
and see whether they still hold in the presence of our library

\subsection{Type safety}

Certainly purity ought to imply memory and type safety: What we get out of \verb|RS.get| should really be a valid \verb|Set|, and not some corrupted memory.

It is not hard to break type safety with \verb|unsafePerformIO|, especially when there are mutable references and polymorphism around. Nevertheless, we are fairly confident that our library is type and memory safe.

\subsection{No (observable) side effects}

Our library uses \verb|IO| inside, and clearly we would not consider it to be pure if inserting \verb|42| into a set would make the computer eject the CD drive. Avoiding these blatant side effects is simple -- we just don't do it.

However, \verb|RS.insert 42 rs| \emph{does} destructively modify mutable data structures inside \verb|rs|. It is crucial that these changes stay internal to the library, and are not observable by the programmer. It must not affect the result the programmer gets via \verb|RS.get rs|, nor any other \emph{observable} value. Again, we believe that this is the case.

That the space leak issue of the naive implementation described in \cref{sec:spaceleak} is almost an issue of this kind: If we’d include space leaks in our notion of “observable behavior”, then that would be an example of such a side-effect issue.

\subsection{Evaluation order independence.}

Even more, it should not matter in which order the various subexpressions are evaluated. In
\begin{code}
ghci> let  s1 = RS.insert 42 s2
ghci|      s2 = RS.insert 23 s3
ghci|      s3 = RS.delete 42 s1
ghci| in (RS.get s1, RS.get s2, RS.get s3)
\end{code}
we need to get the same result, no matter whether we evaluate the first component of the resulting tuple before the second, or the other way around, or even all in parallel in separate threads.

Achieving this in our library required some care. Because we wrap our computation in \verb|unsafePerformIO|, we have to expect any of them to happen at almost any time. Simple graph traversal methods, which simply mark a node as “done” before visiting its successors, easily go wrong when a second evaluation hits such a supposedly “done” node, while the first traversal is still processing the graph. Getting this to the point where it seems to be working was non-trivial, as mentioned in \cref{sec:thread}.

\subsection{Referential transparency}

Purity is usually understood to also imply referential transparency: If \verb|x| is defined to be \verb|e|, then we can replace an occurrence \verb|x| with \verb|e|, and vice-versa, without changing the meaning of the program.

Care has to be taken when interpreting this definition in recursive definitions. Of course we do not expect \verb|let x = 1 in x| to be the same as \verb|let x = x in x|, although we merely replaced \verb|1| with \verb|x|.

So is our library still referential transparent? By virtue of always calculating a unique fixpoint, it is!  The value of \verb|let rs2 = RS.insert 42 rs1| is the (least) set for which the equation \verb|RS.get rs2 == S.insert 42 (RS.get rs1)| is true, and thus \verb|rs2| and \verb|RS.insert 42 rs1| can be used interchangeable.

This restriction to fixpoints, i.e.\ to solutions of a set of equations, puts a clear limit on the kind of observations we allow the programmer to make about the recursive equations. For example, it would not be pure if the programmer could obtain the number of equations. This rules out using our approach for use-cases where the actual graph structure of the equations (and not just a solution of the equations) is of interest, e.g.\ generating circuit descriptions \cite{observable-sharing}.

\subsection{Program equivalences and lambda lifting}\label{sec:sat}

Referential transparency is a crucial ingredient to equational reasoning: rewriting the code according to a set of rules, while preserving its meaning. Replacing \verb|x| with its definition \verb|e| is called unfolding, or \textdelta-reduction, but there are further program transformations that we expect to preserve semantics. They may be applied by optimizing compiler, used in program verification or simply by a programmer trying to understand the meaning of the code.

Ideally, all program transformations that are meaning-preserving in pure Haskell ould still meaning-preserving in the presence of our library. Unfortuately, this is not the case: Lambda lifting can turn terminating programs into non-terminating ones.

In general in Haskell, if we have a recursive value definition involving an expression \verb|e|, we it is ok to turn that into a recursive function definition abstracting over that expression: We can go from
\begin{code}
let x = ... x ... e ... in x
\end{code}
to
\begin{code}
let x y = ... (x y) ... y ... in x e
\end{code}
without changing the program's meaning. With our library, this can now make the difference between termination and non-termination:
\begin{code}
ghci> let rs = RS.insert 42 rs in RS.get rs
fromList [42]
ghci> let rs y = RS.insert y (rs y) in RS.get (rs 42)
fromList ^CInterrupted.
\end{code}

This is not a simple infelicity of our implementation, but a fundamental limitation: In the original program, our library can get its (magic, \verb|unsafePerformIO|-powered) hands on a \emph{finite} graph of connected values. In the transformed program there is now a recursive function that endlessly creates new values, and our library can never see a complete set of equations. It seems that no (reasonable) implementation of the interface in \cref{fig:api} can solve this problem. By this reasonsing we deduce that a pure implementation of that interface indeed does not exist.

So does this invalidate our claims that we built a beautiful, well-behaved, pure abstraction? In the end that depends on your expectation: Do you expect that in a lazy, pure language lambda-lifting preserves meaning, including termination? Or is that an additional property of a language, which we can maybe do without?

As you ponder this question, note that if you include (asymptotic) resource usage in your expectation of program equivalence, then such lambda-lifting is already rather dangerous in Haskell: The nice and fast knot-tied \verb|fibs| from the introduction becomes horribly inefficient once you turn it into a recursive \emph{function} (\verb|let fibs x y = x : y  : ...|). Because the lamda-lifting transformation is only a equivalence as long as we ignore (asymptotic) complexity, it seems that breaking that equivalence is not too wicked.

\Citet[Fact 3.7]{sabry} points out that merely changing the set of observational equivalences does , in generaly, \emph{not} imply that the extended language is not pure.

\subsection{Compiler transformation}

The compiler tends to share the attitude that (at least asymptotic) complexity is a relevant aspect of program semantics, and will be careful to preserve (recursive) sharing that is explicit in the source program. Therefore we can be fairly certain that it will not just nilly-willy apply such lambda-lifting to our code.

The same holds for all the other transformations applied by the compiler, such as partial evaluation, case-of-case, worker/wrapper, common subexpression elimination etc.\ (\cite{optimiser}). Because our library has nice equational properties, as long as sharing is preserved, we can be fairly certain that all these are still meaning-preserving in the presence of our library.


\subsection{Sabry's criterion}

One possible definition for purity is offered by \citet{sabry}:
\begin{quote}
A language is purely functional if (i) it includes every simply typed λ-calculus term, and
(ii) its call-by-name, call-by-need, and call-by-value implementations are equivalent (modulo
divergence and errors).
\end{quote}

It is not straight-forward to apply this criterion to our library, or Haskell itself, because it assumes the existence of these three implementations, and we do not have that for Haskell. Even if we had, in the call-by-value implementation, all our recursive definitions would vacuously loop, and in the call-by-name implementation (quite like after the lambda lifting), our library would not see \emph{finite} graphs, so all interesting programs would become no-terminating. In that sense, our library \emph{is} pure according to Sabry’s criterion.

Because it requires multiple implementations we find Sabry’s criterion of purity not fully convincing. Maybe there exists a suitable criterion that works with a single language semantics, maybe by listing observable equivalences that must hold for a language to be considered pure?
We'll leave this question hanging, hoping it can tempt someone to give a satisfying answer.

% \subsection{Denotational semantics}
% 
% Pure functional programs can be given meaning not just via their actual implementation, or the pen-and-paper equivalent, their operational semantics, but also via denotaional semantics. These map every language construct compositionally to a mathematical model.


\subsection{How to prove it?}

So with the exception of preservation under lambda-lifting, we are fairly confident that our library is as pure as advertised. Ideally we'd like to be sure, and we’d be sure if we could perform a rigorous proof.  That not only requires a clear definition of what exactly “pure“ means, as discussed above, but also a framework that allows us to to reason about lazy programs with \verb|unsafePerformIO|, \verb|IORef|, laziness and (ideally) multiple threads.

Unfortunately, we do not know how to do that well. In the single-threaded world we could imagine extending an operational semantics for lazy languages such as \citeauthor{launchbury}’s [\citeyear{launchbury}] natural semantics for lazy evaluation with a global store for the mutable variables, and spurious evaluations for \verb|unsafePerformIO|, but it seems tedious and we have not even added threads.

The maybe most similar work is \citeauthor{runST}'s \citeyear{runST} proof that \verb|runST :: ST -> a| is safe, but it does not yet cover laziness nor concurrency.
They used the Iris framework, and for Idris one even finds a mechanized proof of the correctness of a generic fixpoint solver \citep{spygame}.
It seems that important building blocks are already there, and we hope that this paper can maybe spurn motivate more work in that direction.


\section{Related work}\label{sec:related}

The present work draws inspiration from and connects to various directions.

\subsection{Fixpoint solvers and propagation networks}

\Cref{sec:impcore} describes the “imperative core” of our library: An underlying library that allows us to declare the cells of a possibly cyclic graph of values and their relations and finds the solution. For the purposes of this paper we can assume this to be a black box, to avoid getting distracted by questions related to the theory and implementation of this black box, which is good, as there is much interesting than can be said and done here.

All the work in making fixpoint solvers and data-flow analyses efficient is relevant here \citep{kildall-73,kam-ullman-76}. An important difference to typical data-flow analyses is that in order to stay pure, we cannot afford to make conservative approximations while solving the equations (e.g.\ aborting with a safe guess after a certain number of iterations).

Also related is the concept of \emph{propagator network} \citep{propagator}, although our use-case is a bit simpler, as information only flows in one direction along an edge, and once we queried the value of one node, we never add more constraints that would influence that node.

%\subsection{LVars}
%
% One might wonder if Lindsey Kuper's LVar library can be used to implement

\subsection{Shallow graph embeddings and observable sharing}

\Citet{observable-sharing} made sharing observable in Haskell, so that they can very elegantly describe logic circuits in Haskell. They extend pure Haskell with operations that one might dismiss as impure (observable object identity), point out that this breaks some equational equivalences of Haskell, but that this may be acceptable, given that some of these code transformations are not benign to begin with, as they can duplicate arbitrary amount of work anyways -- a line of reasoning you may recognize from \cref{sec:pure}.

Their extension to Haskell is, in a way, bolder: It breaks more laws (referential transparency) in order to make the structure of the embedded graph observable, as necessary for their poster use-case. With our library, you \emph{cannot} observe the structure of the graph; you only get your hands on a unique fixpoint. This means it is unsuitable for some use-cases, but also less intrusive.

We probably could have implemented our library on top of their observable sharing mechanism, reifying the graph of values as a data structure with explicitly named. We chose to do it differently, and use Haskell's laziness together with \verb|unsafePerformIO| (which will be executed once per value) to create unique mutable cells and let those cells refer to each other. The graph is never actually turned into a (Haskell-level) data structure and the cells are not (explicitly) numbered or named; instead the graphs is represented as pointers on the heap, cells are implicitly identified by their object identity, and the bookkeeping data for each cell is stored within its mutable fields.

Looking beyond Haskell we want to point out CoCaml \citep{cocaml}, which allows the Ocaml programmer to observe the structure of knot-tied (coinductive) data, and even process such data while preserving the sharing (so that a \verb|map| over a cyclic list is still cyclic).


\subsection{Logic programming}

For the kind of use-cases presented here, logic programming languages like Prolog and Datalog are certainly on their home turf. What we bring to the table is the seamless integration into a purely functional language.

Particularly close to our work, and straddling the functional and logic programming paradigms, is the \emph{Datafun} language \citep{datafun}, a pure and total functional programming language generalizing Datalog, which can declaratively express and compute fixed points of monotone maps on semilattices -- exactly what we are trying to do. Since they tailor their language around this idea, their type system can recognize \emph{monotonic} function definitions.

Monotonicity is crucial for the existence and uniqueness of a solution (\cref{sec:monotonicity}). We ensure monotonicity by simply restricting the interface (\cref{fig:api}) to only expose monotonic functions. This works, but is rather limiting when it comes to higher-order functions. Assume we would want to support recursively-definable finite \emph{maps} as well. A natural order on finite maps is the point-wise ordering. But with that ordering, higher-order operations like \verb|map :: (a -> b) -> RMap k a -> RMap k b| are only monotonic if their argument is monotonic, and Haskell's type system does not allow us to express that constraint. Without these higher-order functions, however, the map API would be quite impoverished, so we do not support finite maps with the point-wise ordering in our library.\footnote{You might notice the \texttt{Data.Recursive.Map} module in the package. This implements maps with the \emph{discrete} ordering on values, where all higher order function arguments are vacuously monotonic, thus avoiding this issue.} Datafun's type system has a separate function arrow $\xrightarrow{+}$ to characterize monotonic functions, and thus supports this use-case quite well.


\bibliographystyle{ACM-Reference-Format}
\bibliography{bib}

\clearpage
\appendix
\section{The reflexivie transitive closure, pedestrian-style}\label{sec:imp}

In the introduction we mentioned that without the data structure presented in this paper, a programmer likely has to write their reflexive-transitive-closure code with an explicit loop (a tail-recursive \verb|go| function), keeping track of seen nodes:
\begin{code}
transitiveImp :: Graph -> Graph
transitiveImp g = M.fromList [ (v, S.toList (go S.empty [v])) | v <- M.keys g ]
  where
    go :: S.Set Int -> [Int] -> S.Set Int
    go seen [] = seen
    go seen (v:vs)  |  v `S.member` seen  =  go seen vs
                    |  otherwise          =  go (S.insert v seen) (g M.! v ++ vs)
\end{code}


\end{document}
